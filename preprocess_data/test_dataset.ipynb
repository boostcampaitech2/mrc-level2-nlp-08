{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, TensorDataset, SequentialSampler\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = re.sub(r\"\\n\", \" \", text)\n",
    "    text = re.sub(r\"\\\\n\", \" \", text)  # remove newline character\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # remove continuous spaces\n",
    "    text = re.sub(r\"#\", \" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensor_for_dense_negative(\n",
    "    data_path: str,\n",
    "    bm25_path: str,\n",
    "    max_context_seq_length: int,\n",
    "    max_question_seq_length: int,\n",
    "    tokenizer,\n",
    ") -> TensorDataset:\n",
    "# 0번 idx - > ground truth ( 전처리)\n",
    "# 1~50 idx -> bm25 negative\n",
    "\n",
    "    dataset = load_from_disk(data_path).to_pandas()\n",
    "    \n",
    "    # ctx = []\n",
    "    # print(dataset[\"context\"][0])\n",
    "    # for i in tqdm(range(len(dataset))):\n",
    "    #     ctx.append(preprocess(dataset[\"context\"][i]))\n",
    "    dataset[\"context\"] = dataset[\"context\"].apply(preprocess)\n",
    "    ctx = dataset[\"context\"].to_list()\n",
    "    q_list = dataset[\"question\"].to_list()\n",
    "    # 시간이 많이 걸림 -> 미리 처리해둘것\n",
    "    #print(ctx[0:1])\n",
    "    with open(bm25_path, \"rb\") as file:\n",
    "        elastic_pair = pickle.load(file)\n",
    "    neg_ctx = []\n",
    "\n",
    "    num_neg = 2\n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        #print(i)\n",
    "        query = dataset[\"question\"][i]\n",
    "        ground_truth = ctx[i]\n",
    "        answer = dataset[\"answers\"][i][\"text\"][0]\n",
    "        cnt = num_neg\n",
    "        idx = 0\n",
    "        while cnt != 0:\n",
    "            if ground_truth != elastic_pair[query][idx] and not (answer in elastic_pair[query][idx]):\n",
    "                # 비슷한 context를 추가하되 정답을 포함하지 않는 문장을 추가한다.\n",
    "                neg_ctx.append(elastic_pair[query][idx])\n",
    "                cnt -= 1\n",
    "            idx += 1\n",
    "            \n",
    "            if idx == 200:  # index를 넘어가면 마지막에 추가된 context를 채워줌\n",
    "                #print('in', cnt)\n",
    "                while(cnt != 0):\n",
    "                    neg_ctx.append(elastic_pair[query][-1])\n",
    "                    cnt -= 1\n",
    "                    #print(cnt)\n",
    "                #print(i)\n",
    "                break\n",
    "    \n",
    " \n",
    "    q_seqs = tokenizer(\n",
    "        q_list,\n",
    "        max_length=max_question_seq_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    p_seqs = tokenizer(\n",
    "        ctx,\n",
    "        max_length=max_context_seq_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    print(p_seqs['input_ids'].size())\n",
    "    np_seqs = tokenizer(\n",
    "        neg_ctx,\n",
    "        max_length=max_context_seq_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    max_len = np_seqs['input_ids'].size(-1)\n",
    "    np_seqs['input_ids'] = np_seqs['input_ids'].view(-1 , num_neg, max_len)\n",
    "    np_seqs['attention_mask'] = np_seqs['attention_mask'].view(-1, num_neg, max_len)\n",
    "    np_seqs['token_type_ids'] = np_seqs['token_type_ids'].view(-1, num_neg, max_len)\n",
    "    \n",
    "    tensor_dataset = TensorDataset(\n",
    "        (p_seqs[\"input_ids\"], np_seqs[\"input_ids\"]),\n",
    "        (p_seqs[\"attention_mask\"],np_seqs[\"attention_mask\"]),\n",
    "        (p_seqs[\"token_type_ids\"],np_seqs[\"token_type_ids\"]),\n",
    "        q_seqs[\"input_ids\"],\n",
    "        q_seqs[\"attention_mask\"],\n",
    "        q_seqs[\"token_type_ids\"],\n",
    "    )\n",
    "    # neg_tensor_dataset = TensorDataset(\n",
    "    #     np_seqs[\"input_ids\"],\n",
    "    #     np_seqs[\"attention_mask\"],\n",
    "    #     np_seqs[\"token_type_ids\"],\n",
    "    # )\n",
    "\n",
    "    return tensor_dataset #, neg_tensor_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InBatchNegativeRandomDataset(Dataset):\n",
    "    def __init__(self, data_path: str,bm25_path: str,max_context_seq_length: int,max_question_seq_length: int,neg_num,tokenizer):\n",
    "        preprocess_data = self.preprocess_pos_neg(data_path, bm25_path, max_context_seq_length, max_question_seq_length,neg_num, tokenizer)\n",
    "        \n",
    "        self.p_input_ids = preprocess_data[0]\n",
    "        self.p_attension_mask = preprocess_data[1]\n",
    "        self.p_token_type_ids = preprocess_data[2]\n",
    "        \n",
    "        self.np_input_ids = preprocess_data[3]\n",
    "        self.np_attension_mask = preprocess_data[4]\n",
    "        self.np_token_type_ids = preprocess_data[5]\n",
    "        \n",
    "        self.q_input_ids = preprocess_data[6]\n",
    "        self.q_attension_mask = preprocess_data[7]\n",
    "        self.q_token_type_ids = preprocess_data[8]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.p_input_ids.size()[0]\n",
    "    def __getitem__(self, index):\n",
    "        return (self.p_input_ids[index],self.p_attension_mask[index],self.p_token_type_ids[index],\n",
    "        self.np_input_ids[index],self.np_attension_mask[index],self.np_token_type_ids[index],\n",
    "        self.q_input_ids[index],self.q_attension_mask[index],self.q_token_type_ids[index])\n",
    "\n",
    "    def preprocess_pos_neg(self, data_path: str,bm25_path: str,max_context_seq_length: int,max_question_seq_length: int, neg_num,tokenizer):\n",
    "        dataset = load_from_disk(data_path).to_pandas()\n",
    "        dataset[\"context\"] = dataset[\"context\"].apply(self.preprocess_text)\n",
    "        ctx = dataset[\"context\"].to_list()\n",
    "        q_list = dataset[\"question\"].to_list()\n",
    "        with open(bm25_path, \"rb\") as file:\n",
    "            elastic_pair = pickle.load(file)\n",
    "        neg_ctx = []\n",
    "        num_neg = neg_num\n",
    "        for i in tqdm(range(len(dataset))):\n",
    "            #print(i)\n",
    "            query = dataset[\"question\"][i]\n",
    "            ground_truth = ctx[i]\n",
    "            answer = dataset[\"answers\"][i][\"text\"][0]\n",
    "            cnt = num_neg\n",
    "            idx = 0\n",
    "            while cnt != 0:\n",
    "                if ground_truth != elastic_pair[query][idx] and not (answer in elastic_pair[query][idx]):\n",
    "                    # 비슷한 context를 추가하되 정답을 포함하지 않는 문장을 추가한다.\n",
    "                    neg_ctx.append(elastic_pair[query][idx])\n",
    "                    cnt -= 1\n",
    "                idx += 1\n",
    "                if idx == 200:  # index를 넘어가면 마지막에 추가된 context를 채워줌\n",
    "                    #print('in', cnt)\n",
    "                    while(cnt != 0):\n",
    "                        neg_ctx.append(elastic_pair[query][-1])\n",
    "                        cnt -= 1\n",
    "                    break\n",
    "        q_seqs = tokenizer(\n",
    "            q_list,\n",
    "            max_length=max_question_seq_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        p_seqs = tokenizer(\n",
    "            ctx,\n",
    "            max_length=max_context_seq_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        np_seqs = tokenizer(\n",
    "            neg_ctx,\n",
    "            max_length=max_context_seq_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        max_len = np_seqs['input_ids'].size(-1)\n",
    "        np_seqs['input_ids'] = np_seqs['input_ids'].view(-1 , num_neg, max_len)\n",
    "        np_seqs['attention_mask'] = np_seqs['attention_mask'].view(-1, num_neg, max_len)\n",
    "        np_seqs['token_type_ids'] = np_seqs['token_type_ids'].view(-1, num_neg, max_len)\n",
    "\n",
    "        return (p_seqs['input_ids'], p_seqs['attention_mask'], p_seqs['token_type_ids'], \n",
    "        np_seqs['input_ids'], np_seqs['attention_mask'],np_seqs['token_type_ids'],\n",
    "        q_seqs['input_ids'],q_seqs['attention_mask'],np_seqs['token_type_ids'])\n",
    "    def preprocess_text(self,text):\n",
    "        text = re.sub(r\"\\n\", \" \", text)\n",
    "        text = re.sub(r\"\\\\n\", \" \", text)  # remove newline character\n",
    "        text = re.sub(r\"\\s+\", \" \", text)  # remove continuous spaces\n",
    "        text = re.sub(r\"#\", \" \", text)\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3952/3952 [00:00<00:00, 43366.16it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = InBatchNegativeRandom(\n",
    "    \"/opt/ml/data/train_dataset/train\",\n",
    "    '/opt/ml/data/elastic_new_train_500.bin',\n",
    "    512,\n",
    "    64,\n",
    "    5,\n",
    "    tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3952"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample size, emd size\n",
    "# sample size * neg cnt, emb size\n",
    "\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1,p2,p3,n1,n2,n3,q1,q2,q3 = train_dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset,batch_size=16,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 512])\n",
      "torch.Size([16, 5, 512])\n",
      "torch.Size([16, 512])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch[0].size())\n",
    "    # print(batch[1])\n",
    "    # print(batch[2])\n",
    "\n",
    "    neg_batch_ids = []\n",
    "    neg_batch_att = []\n",
    "    neg_batch_tti = []\n",
    "    neg_random_idx = random.randrange(0,5)\n",
    "    for i in range(16): # 배치만큼\n",
    "        neg_batch_ids.append(batch[3][:][i][neg_random_idx].unsqueeze(0))\n",
    "        neg_batch_att.append(batch[4][:][i][neg_random_idx].unsqueeze(0))\n",
    "        neg_batch_tti.append(batch[5][:][i][neg_random_idx].unsqueeze(0))\n",
    "    print(batch[3].size())\n",
    "    #print(batch[3].size()[-2]) # 랜덤 index 범위\n",
    "    # print(batch[3][:][2])\n",
    "    # print(batch[3][:][3])\n",
    "    neg_batch_ids = torch.cat(neg_batch_ids)\n",
    "    neg_batch_att = torch.cat(neg_batch_att)\n",
    "    neg_batch_tti = torch.cat(neg_batch_tti)\n",
    "\n",
    "    print(neg_batch_ids.size())\n",
    "\n",
    "    # print(batch[4][:][0])\n",
    "    # print(batch[5][:][0])\n",
    "\n",
    "    # print(batch[6])\n",
    "    # print(batch[7])\n",
    "    # print(batch[8])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.randrange(0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
