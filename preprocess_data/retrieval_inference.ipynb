{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from datasets import load_from_disk, Dataset\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from retrieval_model import BertEncoder\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import (DataLoader, RandomSampler, TensorDataset, SequentialSampler)\n",
    "from tqdm import tqdm, trange\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DPR model load\n",
    "p_encoder = BertEncoder.from_pretrained(\"/opt/ml/mrc-level2-nlp-08/retrieval/p_encoder\")\n",
    "q_encoder = BertEncoder.from_pretrained(\"/opt/ml/mrc-level2-nlp-08/retrieval/q_encoder\")\n",
    "tokenizer =  AutoTokenizer.from_pretrained(\"klue/bert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset = load_from_disk(\"/opt/ml/data/train_dataset/train/\")\n",
    "#query_dataset = load_from_disk(\"../data/test_dataset/validation\") # test query\n",
    "#wiki_dataset = pd.read_json('../data/wikipedia_documents.json',orient='index') # wiki context\n",
    "#wiki_dataset = pd.read_csv('/opt/ml/data/preprocess_wiki_doc.csv')\n",
    "#train_dataset = load_from_disk(\"/opt/ml/data/train_dataset/new_validation/\")\n",
    "#origin_valid = load_from_disk(\"/opt/ml/data/train_dataset/validation/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_dataset = pd.read_csv('/opt/ml/data/preprocess_wiki_doc.csv')\n",
    "query_dataset = load_from_disk(\"/opt/ml/data/train_dataset/validation/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num = 7\n",
    "# print(train_dataset['question'][num])\n",
    "# print(origin_valid['context'][num])\n",
    "# print('---------------')\n",
    "# print(train_dataset['context'][num])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_corpus = list(set([example for example in train_dataset['context']]))\n",
    "wiki_corpus = list(set([example_wiki for example_wiki in wiki_dataset['text']]))\n",
    "query = query_dataset['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240\n",
      "55963\n"
     ]
    }
   ],
   "source": [
    "print(len(query))\n",
    "print(len(wiki_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 1749/1749 [09:40<00:00,  3.02it/s]\n"
     ]
    }
   ],
   "source": [
    "eval_batch_size = 32\n",
    "def to_cuda(batch):\n",
    "  return tuple(t.cuda() for t in batch)\n",
    "if torch.cuda.is_available():\n",
    "    p_encoder.cuda()\n",
    "    q_encoder.cuda()\n",
    "\n",
    "# Construt dataloader\n",
    "train_p_seqs = tokenizer(wiki_corpus, padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "valid_dataset = TensorDataset(\n",
    "    train_p_seqs[\"input_ids\"],\n",
    "    train_p_seqs[\"attention_mask\"],\n",
    "    train_p_seqs[\"token_type_ids\"]\n",
    ")\n",
    "valid_sampler = SequentialSampler(valid_dataset)\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    sampler=valid_sampler,\n",
    "    batch_size=eval_batch_size\n",
    ")\n",
    "\n",
    "# Inference using the passage encoder to get dense embeddeings\n",
    "p_embs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    epoch_iterator = tqdm(\n",
    "        valid_dataloader,\n",
    "        desc=\"Iteration\",\n",
    "        position=0,\n",
    "        leave=True\n",
    "    )\n",
    "    p_encoder.eval()\n",
    "\n",
    "    for _, batch in enumerate(epoch_iterator):\n",
    "        batch = tuple(t.cuda() for t in batch)\n",
    "\n",
    "        p_inputs = {\n",
    "            \"input_ids\": batch[0],\n",
    "            \"attention_mask\": batch[1],\n",
    "            \"token_type_ids\": batch[2]\n",
    "        }\n",
    "        \n",
    "        outputs = p_encoder(**p_inputs).to(\"cpu\").numpy()\n",
    "        p_embs.extend(outputs)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 8/8 [00:00<00:00, 44.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_q_seqs = tokenizer(\n",
    "    query,\n",
    "    max_length=30,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "query_dataset = TensorDataset(\n",
    "    train_q_seqs[\"input_ids\"],\n",
    "    train_q_seqs[\"attention_mask\"],\n",
    "    train_q_seqs[\"token_type_ids\"]\n",
    ")\n",
    "\n",
    "query_sampler = SequentialSampler(query_dataset)\n",
    "query_dataloader = DataLoader(\n",
    "    query_dataset,\n",
    "    sampler=query_sampler,\n",
    "    batch_size=eval_batch_size\n",
    ")\n",
    "\n",
    "q_embs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    epoch_iterator = tqdm(\n",
    "        query_dataloader,\n",
    "        desc=\"Iteration\",\n",
    "        position=0,\n",
    "        leave=True\n",
    "    )\n",
    "    q_encoder.eval()\n",
    "\n",
    "    for _, batch in enumerate(epoch_iterator):\n",
    "        batch = tuple(t.cuda() for t in batch)\n",
    "\n",
    "        q_inputs = {\n",
    "            \"input_ids\": batch[0],\n",
    "            \"attention_mask\": batch[1],\n",
    "            \"token_type_ids\": batch[2]\n",
    "        }\n",
    "        \n",
    "        outputs = q_encoder(**q_inputs).to(\"cpu\").numpy()\n",
    "        q_embs.extend(outputs)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55963, 768)\n",
      "(240, 768)\n"
     ]
    }
   ],
   "source": [
    "p_embs = np.array(p_embs)\n",
    "q_embs = np.array(q_embs)\n",
    "\n",
    "print(p_embs.shape)\n",
    "print(q_embs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    p_embs_cuda = torch.Tensor(p_embs).to('cuda')\n",
    "    q_embs_cuda = torch.Tensor(q_embs).to('cuda')\n",
    "\n",
    "dot_prod_scores = torch.matmul(q_embs_cuda, torch.transpose(p_embs_cuda, 0, 1))\n",
    "rank = torch.argsort(dot_prod_scores, dim=1, descending=True).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240/240 [00:00<00:00, 479.19it/s]\n"
     ]
    }
   ],
   "source": [
    "dense_p_retrieval_result = {}\n",
    "idx = 0\n",
    "for i in tqdm(range(len(query))):\n",
    "    p_list = []\n",
    "    q = query[i]\n",
    "    for j in range(100):\n",
    "        p_list.append(wiki_corpus[rank[idx][j]])\n",
    "    dense_p_retrieval_result[q] = p_list\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/opt/ml/data/dense_valid_retrieval.bin\", \"wb\") as file:\n",
    "    pickle.dump(dense_p_retrieval_result,file)\n",
    "\n",
    "\n",
    "# dense_n_retrieval_result = {}\n",
    "# idx = 0\n",
    "# for i in tqdm(range(len(query))):\n",
    "#     p_list = []\n",
    "#     q = query[i]\n",
    "#     for j in range(10000,10004):\n",
    "#         p_list.append(wiki_corpus[rank[idx][j]])\n",
    "#     dense_n_retrieval_result[q] = p_list\n",
    "#     idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "마오를 직접적으로 죽인 사람은?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"1925년 3월, 쑨원이 사망하자 그의 추도식에 참석하였다. 그 해 5월 말경에 조봉암이 여운형의 집에 찾아와서 조선공산당이 제3인터내셔널(코민테른)의 승인을 받아야 하니 모스크바로 가는 여권을 받게 도와달라고 부탁했고, 여운형은 이를 도와주었다. 이는 후에 신의주 사건(제1차 조선공산당검거사건)과 맞물려서 물의를 빚게 되었다. 5월 30일 영국 경찰이 불평등 조약에 반대하는 중국인 젊은이 20여 명을 살해한 '5.30 사건' 이후 중국 혁명에도 참가하게 되었다 반영 시위는 1926년 10월까지 지속되었다 이 기간 중 그는 중국국민당 대표 사오리쯔와 공산당 대표 취추바이 등과 함께 운동의 방향에 대해 일주일에 두세 번 이상 만나 토의하기도 하였다 1925년부터 1927년까지는 상하이에서 소련이 새로 시작한 타스 통신사에 취직, 근무한다 1926년 1월에 의열단 단장으로 무장 항일투쟁한 김원봉을 황포 군관학교로 입학시키는데 도왔다. 이와 동시에 1926년 1월, 국민당 2차 대표대회에 국민당 정부 주석 왕징웨이의 초대로 참석하여 연설을 하였으 보로딘과 함께 왕징웨이의 자문활동도 하였다 1927년 장제스가 대대적인 공산당 숙청을 하면서 그의 중국 혁명 활동도 중단되었다 중국 혁명 활동과 조선독립운동의 활동이 여의치 않게 되자, 중국 복단 대학교 체육교사로 취직하였고, 수학여행차 학생들을 데리고 동남아시아로 건너가 싱가포르, 필리핀 일대에서 '제국주의에 대항하는 투쟁, 민족해방을 촉구'하는 내용의 연설을 하였다. 이 때문에 필리핀 지역에서 경찰에 의해 강제로 억류되기도 했고, 싱가포르 지역의 영국 경찰들과 마찰이 생겨 여권을 빼앗기거나 쫓겨나기도 했다\",\n",
       " '산둥성의 부유한 지주집안 출신으로 본명은 장숙평(張叔平)이다. 1924년 상하이 대학에 들어가서 사회학을 공부하였고, 1925년 중국공산당에 가입했다. 이후 상하이 지역에서 노동운동과 지하운동에 종사했다. 1931년 중앙당의 조직부장으로 임명되어, 이후 당의 보안과 정보 조직의 수장이 되었다. 1933년 당에 의해 모스크바로 파견되어 소련의 보안과 정보에 관련된 기술을 공부하였다. 이때 왕밍과 알게되었고, 왕밍의 원조로 중국공산당의 중앙위원회의 위원이 되었다. 이때 한창이던 스탈린의 숙청을 흉내내어 소련내의 중국공산당 멤버 중 성향이 의심스러운 자들을 트로츠키주의자로 낙인찍어 강제수용소(굴라그)로 보내거나 처형하기도 하였다. 1935년 이름을 캉셩으로 바꾸고 중국공산당의 본부가 있던 옌안으로 돌아왔다. 이때 재빨리 권력이동의 흐름을 눈치채고 왕밍을 떠나 마오쩌둥의 편에 서서 왕밍을 공격하는 데 앞장섰다. 마오는 그 보답으로 그를 당의 보안을 책임진 사회부의 수장으로 앉혔다. 그는 이 조직을 맡아 일본이나 국민당의 첩자로 낙인찍어 수많은 인사들을 처형하였고, 그중에는 님 웨일즈의 저서 아리랑의 주인공인 조선인 독립투사 김산도 있었다. 1942년부터 마오와 캉은 당에 마오사상을 뿌리내리기 위한 정풍운동을 주도하여 왕밍과 장궈타오의 잔당을 뿌리뽑으려고 했고, 이에 따라 수많은 인사들이 스파이, 배신자의 명목으로 체포나 고문을 당하고 처형되었다. 캉이 옌안에서 이끈 이러한 \"적색테러\"는 너무 지나치고 광범위했기 때문에, 이후에 캉은 공개적으로 사죄하고 한직으로 좌천되었지만, 나름대로 보안에 기여하여 1930년대 빈번했던 공산당의 정보누출은 거의 없어졌다. 그 결과 국민당측에서 옌안의 전략전술은 거의 알아차릴 수 없어 공산당의 최종승리에 기여했다. 또한 같은 산둥 출신이던 장칭을 마오에게 소개하였고, 장칭과 마오는 후에 결혼하였다. 산둥 지역으로 좌천된 캉은 국공내전 기간 동안 그지역의 토지개혁을 주도하였지만, 중화인민공화국 성립 이후에도 류샤오치, 저우언라이의 견제와 나쁜건강 때문에 좀처럼 권력의 핵심으로 나올 수 없었다. 1959년 펑더화이의 실각과 관련된 \"반우파투쟁\"과 1960년 중소 공산당의 결렬로 인해 다시 마오의 신임을 받아 권력을 핵심에 진입하였다. 이후 공안기관인 중국공산당 중앙조사부장을 맡아 장칭과 함께 문화대혁명을 주도하였고, 공포분위기 조성을 위해 \"내몽골인민당 사건\"과, \"윈난성 당서기 자오쩬민 스파이사건\"을 조작하여 수많은 인사들을 죽음으로 내몰았고, 류샤오치,덩샤오핑,펑더화이의 박해에도 관여하였다. 캉은 옌안시절의 좌천을 교훈삼아 이러한 박해작업에서 직접 손에 피를 묻히지 않고 홍위병의 손에 맡겨 책임을 교묘히 회피하였고, 그에 대한 마오의 신임도 높아져서 린뱌오의 몰락 이후 1974년에는 당부주석에 취임하고, 당서열 4위까지 오르기도 하였다. 이후 저우언라이와 덩샤오핑을 겨눈 음모적인 운동을 준비하던 중 1975년 암으로 사망하였다.',\n",
       " '초기 중국의 홍군은 취추바이의 좌경 노선에 따라 소비에트를 건설하고 도시 무장폭동을 계획하였다. 이후 소비에트에 수많은 공산주의자가 집결하면서 소비에트의 운영 문제를 놓고 열띤 논쟁이 생기게 된다. 마오쩌둥은 당시 소비에트 노선을 비판하고 인민혁명정부 노선을 주장하였다. 그는 중국에 존재하는 산업노동자와 빈농은 물론이고, 소농과 영세농을 혁명의 주체로 참여시켜야 한다고 보았다. 또한, 1928년 코민테른에서 중국 혁명을 민주주의 혁명이라고 규정한 것을 근거로, 민족적인 부르주아 지식인까지 혁명 대열에 참가시켜 혁명의 역량을 강화해야 한다고 주장했다. 그는 당시 일부 소비에트 지역에서 행해지던 과격한 토지 국유화 노선을 폐기하고, 토지 개혁을 통한 토지 분배 사업에 집중했다. 그러나 이러한 그의 행동은 중국 공산당 내 소비에트 노선 지지자들을 자극하였다. 특히, 소비에트 노선을 지지하고 있던 소비에트 연방 유학파 집단인 28인의 볼셰비키는 마오쩌둥을 권력의 중심부에서 밀어내고, 다시 소비에트 노선을 통해 장시성의 토지 문제를 해결하려고 하였다. 이후 28인의 볼셰비키가 지휘하는 홍군이 국민당군에 패배를 하고 도주를 할 때, 마오쩌둥은 쭌이 회의에서 주도권을 잡았다. 이후 옌안으로 정착한 마오쩌둥은 기존의 인력을 재편하여 인민혁명정부 노선을 관철하였다. 마오쩌둥은 1938년 5월에 《항일전쟁전략》이라는 문건을 통해 혁명 노선에서 정권 문제에 관해 상세히 논하였다. 마오쩌둥은 중국 혁명의 단계가 민주주의 혁명 단계라는 것을 재차 강조하였고, 소비에트는 민족 해방 혁명에서 맞지 않는 정권 형태라고 규정하였다. 마오쩌둥은 자신의 노선을 1940년에 《신민주주의론》라는 문건을 통해 신민주주의혁명 노선이라고 규정하였다. 1942년에는 정풍 운동을 통해 당내 좌경, 우경파를 숙청하여 당내 신민주주의혁명 노선을 강화하였다.',\n",
       " '부하린은 1936년 2월 스탈린의 명령으로 마르크스와 엥겔스의 저작 원고 모음을 구입을 협상하기 위해 파리로 갔다. 이것은 독일 사회민주당이 보유하고 있었으나, 히틀러가 집권한 후 해체되어 프랑스로 옮겨진 것이다. 그는 이때 망명할 기회도 있었으나, 이를 부인하고 \"나는 소련 밖에서는 살수 없다\"고 말했다. 그는 해외활동 중 한때 동지였으나, 10월 혁명후 볼셰비키에 반대하여 망명중인 인사들과 스탈린과 소련에 대한 견해를 나누었다. 특히 과거 멘셰비키로서 이 원고를 관리하고 있던 보리스 니콜라예프스키와 협상할 때 많은 이야기를 했고 이것은 후에 출판된 \"고참 볼세비키의 편지\"의 기초가 되었다. 이 책은 정말 부하린의 저작인지에 대해 논란은 있지만 당시 소련 상황을 이해하는데 좋은 참고자료가 되고 있다. 이 책에서 부하린은 스탈린의 강제집단화 과정에서 벌어진 여러 잔인, 야만행위들을 고발하고 있다. 또한 당내에서는 이에 대한 반대는 전혀 나오지 않고, 오직 굴종만이 최선의 미덕으로 자리잡고 있다고 한탄하였다. 또한 다른 멘세비키 지도자였던 표도르 단과의 대화에서는 \"스탈린은 인간이 아니라 악마지만, 당내에선 무조건 확신을 받고 있고, 당의 상징이 되었다.\"라고 극언을 하기도 하였다. 한편 프랑스의 작가 앙드레 말로와의 대화에서는 부하린은 \"아마 스탈린은 나를 죽일 것이다.\"라고 예언을 하기도 하였다. 또한 이 해외여행이 결국 스탈린이 꾸민 각본이 아닐까 하는 의문을 내비치기도 하였다. 그의 예상대로 이 여행 중의 언행은 모두 그의 반국가활동의 올가미로 작용했다.',\n",
       " '러시아 제국의 모스크바에서 초등학교 교사이던 두 양친 사이에서 태어났다. 그는 16세부터 혁명운동을 시작하였고, 모스크바 대학에 입학한 다음부터도 혁명운동을 계속하였다. 1906년 러시아 사회 민주 노동당에 입당하여 당내 분파인 볼세비키에 가담하였다. 그리고리 소콜리니코프와 함께, 그는 1907년 모스크바에서 전국 청년 연합을 개최하였는데, 이것은 후에 콤소몰의 기원이 되었다. 20살에 그는 당의 모스크바 위원회의 멤버가 되었다. 부하린은 즉각 차르의 비밀경찰인 오하라나의 요주의인물이 되었다. 이 시기에 그는 동지 니콜라이 루킨의 여동생인 나데즈다 루킨을 만나 사랑에 빠졌고, 그가 유형생활 중 결혼하였다. 1911년 부하린은 아르한겔스크로 유형지가 이동되었으나 이곳을 탈출하여 하노버로 망명하였다. 여기서 1년간 머무르다가 다음해 크라쿠프로 가서 레닌을 처음으로 만났다. 그는 망명하면서도 학습을 계속하였고, 20대에 볼셰비키 이론가로 불릴정도로 해박한 지식으로 몇권의 책을 쓰기도 하였다. 특히 그의 저작인 \"제국주의와 세계 경제\"는 레닌이 후에 저술한 저서 \"제국주의-자본주의의 최고단계\"의 토대가 되기도 하였다. 그럼에도 불구하고 그와 레닌은 이론적 문제와 부하린의 친서유럽경향에 대해 크게 논쟁을 할 때도 있었다. 1913년 비인에서 그때까지 잘 알려지지 않았던 그루지아 출신 스탈린의 \"마르크스주의와 민족문제\"의 저술을 돕기도 하였다. 1916년 10월 그는 뉴욕으로 왔고, 트로츠키와 콜론타이와 함께 \"노비이 미르\" (신세계)의 편집진이 되었다.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 209\n",
    "print(query[idx])\n",
    "dense_p_retrieval_result[query[idx]][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_id = load_from_disk(\"../data/test_dataset/validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'question'],\n",
       "    num_rows: 600\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b']\n",
      "[[1], [2]]\n"
     ]
    }
   ],
   "source": [
    "temp = {'a':[1],'b':[2]}\n",
    "\n",
    "print(list(temp.keys()))\n",
    "print(list(temp.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/opt/ml/data/dense_train.bin', \"rb\") as file:\n",
    "    dense_train_retrieval = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(query_dataset[''])\n",
    "dense_train_retrieval[query[0]][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3952/3952 [03:14<00:00, 20.30it/s]\n"
     ]
    }
   ],
   "source": [
    "new_context = []\n",
    "for i in tqdm(range(len(train_dataset))):\n",
    "    train_context = train_dataset['context'][i]\n",
    "    sim_context = dense_train_retrieval[query[i]] # context list\n",
    "    cnt = 4\n",
    "    sim_context_idx = 0\n",
    "    sim_top_k = [train_context] # 정답 context를 제외한 top_k\n",
    "    # add_context = ' '.join(sim_context)\n",
    "    # sim_top_k.append(add_context)\n",
    "    # new_context.append(' '.join(sim_top_k))\n",
    "    while cnt != 0:\n",
    "        if train_context != sim_context[sim_context_idx]:\n",
    "            sim_top_k.append(sim_context[sim_context_idx])\n",
    "            cnt -= 1\n",
    "        sim_context_idx += 1\n",
    "    add_sim_context = ' '.join(sim_top_k)\n",
    "    new_context.append(add_sim_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_dataset.to_pandas()\n",
    "train_df['context'] = new_context\n",
    "new_train_dataset = Dataset.from_pandas(train_df)\n",
    "new_train_dataset.save_to_disk('/opt/ml/data/train_dataset/new_train_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/opt/ml/data/dense_query_wiki_retrieval.bin', \"rb\") as file:\n",
    "#     pickle.dump(dense_p_retrieval_result, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_dataset.to_pandas()\n",
    "train_df['context'] = new_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_origin_df = train_dataset.to_pandas()\n",
    "# all_train = pd.concat([train_df, train_negative_df])\n",
    "# all_train.reset_index(drop=True)\n",
    "# all_train = all_train.drop(['__index_level_0__'],axis=1)\n",
    "# new_train_dataset = Dataset.from_pandas(all_train)\n",
    "# new_train_dataset.save_to_disk('/opt/ml/data/train_dataset/new_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation set\n",
    "train_dataset = load_from_disk(\"/opt/ml/data/train_dataset/validation/\")\n",
    "query = train_dataset['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 15/15 [00:02<00:00,  5.84it/s]\n"
     ]
    }
   ],
   "source": [
    "eval_batch_size = 16\n",
    "train_q_seqs = tokenizer(\n",
    "    query,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "query_dataset = TensorDataset(\n",
    "    train_q_seqs[\"input_ids\"],\n",
    "    train_q_seqs[\"attention_mask\"],\n",
    "    train_q_seqs[\"token_type_ids\"]\n",
    ")\n",
    "\n",
    "query_sampler = SequentialSampler(query_dataset)\n",
    "query_dataloader = DataLoader(\n",
    "    query_dataset,\n",
    "    sampler=query_sampler,\n",
    "    batch_size=eval_batch_size\n",
    ")\n",
    "\n",
    "q_embs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    epoch_iterator = tqdm(\n",
    "        query_dataloader,\n",
    "        desc=\"Iteration\",\n",
    "        position=0,\n",
    "        leave=True\n",
    "    )\n",
    "    q_encoder.eval()\n",
    "\n",
    "    for _, batch in enumerate(epoch_iterator):\n",
    "        batch = tuple(t.cuda() for t in batch)\n",
    "\n",
    "        q_inputs = {\n",
    "            \"input_ids\": batch[0],\n",
    "            \"attention_mask\": batch[1],\n",
    "            \"token_type_ids\": batch[2]\n",
    "        }\n",
    "        \n",
    "        outputs = q_encoder(**q_inputs).to(\"cpu\").numpy()\n",
    "        q_embs.extend(outputs)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3340, 768)\n",
      "(240, 768)\n"
     ]
    }
   ],
   "source": [
    "p_embs = np.array(p_embs)\n",
    "q_embs = np.array(q_embs)\n",
    "\n",
    "print(p_embs.shape)\n",
    "print(q_embs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    p_embs_cuda = torch.Tensor(p_embs).to('cuda')\n",
    "    q_embs_cuda = torch.Tensor(q_embs).to('cuda')\n",
    "\n",
    "dot_prod_scores = torch.matmul(q_embs_cuda, torch.transpose(p_embs_cuda, 0, 1))\n",
    "rank = torch.argsort(dot_prod_scores, dim=1, descending=True).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240/240 [00:00<00:00, 3473.40it/s]\n"
     ]
    }
   ],
   "source": [
    "dense_p_retrieval_result = {}\n",
    "idx = 0\n",
    "for i in tqdm(range(len(query))):\n",
    "    p_list = []\n",
    "    q = query[i]\n",
    "    for j in range(10):\n",
    "        p_list.append(train_corpus[rank[idx][j]])\n",
    "    dense_p_retrieval_result[q] = p_list\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240/240 [00:00<00:00, 304.76it/s]\n"
     ]
    }
   ],
   "source": [
    "new_context = []\n",
    "for i in tqdm(range(len(train_dataset))):\n",
    "    train_context = train_dataset['context'][i]\n",
    "    sim_context = dense_p_retrieval_result[query[i]] # context list\n",
    "    cnt = 4\n",
    "    sim_context_idx = 0\n",
    "    sim_top_k = [train_context] # 정답 context를 제외한 top_k\n",
    "    # add_context = ' '.join(sim_context)\n",
    "    # sim_top_k.append(add_context)\n",
    "    # new_context.append(' '.join(sim_top_k))\n",
    "    while cnt != 0:\n",
    "        if train_context != sim_context[sim_context_idx]:\n",
    "            sim_top_k.append(sim_context[sim_context_idx])\n",
    "            cnt -= 1\n",
    "        sim_context_idx += 1\n",
    "    add_sim_context = ' '.join(sim_top_k)\n",
    "    new_context.append(add_sim_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_dataset.to_pandas()\n",
    "train_df['context'] = new_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처음으로 부실 경영인에 대한 보상 선고를 받은 회사는?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'순천여자고등학교 졸업, 1973년 이화여자대학교를 졸업하고 1975년 제17회 사법시험에 합격하여 판사로 임용되었고 대법원 재판연구관, 수원지법 부장판사, 사법연수원 교수, 특허법원 부장판사 등을 거쳐 능력을 인정받았다. 2003년 최종영 대법원장의 지명으로 헌법재판소 재판관을 역임하였다.\\\\n\\\\n경제민주화위원회(위원장 장하성이 소액주주들을 대표해 한보철강 부실대출에 책임이 있는 이철수 전 제일은행장 등 임원 4명을 상대로 제기한 손해배상청구소송에서 서울지방법원 민사합의17부는 1998년 7월 24일에 \"한보철강에 부실 대출하여 은행에 막대한 손해를 끼친 점이 인정된다\"며 \"원고가 배상을 청구한 400억원 전액을 은행에 배상하라\"고 하면서 부실 경영인에 대한 최초의 배상 판결을 했다. \\\\n\\\\n2004년 10월 신행정수도의건설을위한특별조치법 위헌 확인 소송에서 9인의 재판관 중 유일하게 각하 견해를 내었다. 소수의견에서 전효숙 재판관은 다수견해의 문제점을 지적하면서 관습헌법 법리를 부정하였다. 전효숙 재판관은 서울대학교 근대법학교육 백주년 기념관에서 열린 강연에서, 국회가 고도의 정치적인 사안을 정치로 풀기보다는 헌법재판소에 무조건 맡겨서 해결하려는 자세는 헌법재판소에게 부담스럽다며 소회를 밝힌 바 있다. 호르스트 제호퍼 주총리 밑에서 재무장관을 지냈으며, 주를 대변하는 연방 상원의원으로서 상원 재무위원회 소속이었다.\\\\n\\\\n재무장관 시절 유럽 연합 집행위원회의 일괄 지원을 받고자 부실 주 지원 대출은행인 바이에른LB의 재건을 감독하기도 했다. 2014년에는 바이에른LB를 압박하여 헝가리 측에 MKB 단위를 매각함으로서 20여년 간 20억 유로의 손실을 초래한 부실투자를 종식시키기도 했다. 2015년에는 한스 외르크 셸링 오스트리아 외무장관과 협상을 타결하여 하이포 알페아드리아뱅크 인터내셔널(케른텐주 지역 은행)의 붕괴에서 시작된 양측 정부의 법적 분쟁을 끝냈다. 양해 각서에 따르면 오스트리아는 바이에른주에 12억 3천만 유로를 지불하며, 모든 관련 소송은 취하되었다. \\\\n\\\\n2012년 죄더는 제호퍼 당시 주총리와 함께 연방헌법법원에 소송을 제기하여 바이에른처럼 부유한 주가 전국의 부실경제 구제 차원에서 재정이전을 하도록 하는 독일 시스템 점검을 요구했다. 죄더의 제안에 따라 바이에른주 정부는 독일 최초로 폭스바겐을 상대로 배출가스 시험 사기 사건 관련 소송을 제기해 손해배상을 요구한 주가 되었다. 이 시기 죄더는 해당 스캔들로 인해 70만 유로에 달하는 공무원 연금기금 손실을 입었다고 밝혔다. \\\\n\\\\n2017년 총선 결과 기사련이 참패하면서 제호퍼는 대표직 사퇴 압력을 받게 되었고, 이에 그는 당대표직에서 물러나지는 않는 대신 바이에른주 총리직은 죄더에게 인계하겠다고 밝혔다. 1943년에 경상북도 칠곡군에서 출생하였다. 1970년에 사진가로 입문하여 초기에는 인간의 삶을 다룬 다큐멘터리 사진을 촬영했고, 1989년에 백두산에서 사진 촬영을 하면서 산 사진에 뛰어들어 6개월 동안 산 속에 살면서 작업을 해왔다. 그리고 산 사진 촬영을 통해 터득한 모습으로 높고 험준한 산에서 모습을 드러낸 바 없는 걸작 소나무를 찾아내어 사진에 담고 있었다.\\\\n\\\\n그러나 2011년~2013년 사이에 경상북도 울진군에 소재한 산림유전자원보호구역에서 사진을 촬영하던 도중, 사진 구도에 방해된다는 이유로 200년이 넘은 금강송과 그 외의 나무들을 무단으로 벌목한 것에 대해 논란이 되었다. 이 사건이 언론에 보도되면서 많은 사람들이 분노를 겪였으며, 그는 형사에게 기소되면서 500만원의 벌금형을 선고받았다. 그와 동시에 한국사진작가협회에서 영구제명을 당했으며, 대다수의 환경단체와 사진작가단체에서 사진전 개최에 반대의사는 물론 보이콧까지 일으켰다.\\\\n\\\\n이후 본래 예술의 전당에서 개최하려고 했던 그의 사진전을 미술과 비평에 취소되었다는 소식이 전해졌으나, 이를 상대로 전시회 금지 취소 요청을 하면서 가처분 신청을 냈고, 4월 6일에 서울중앙지법이 이를 받아들여 전시회를 열었다. 이 소식을 들은 환경 단체, 사진 작가 단체, SNS 이용자들이 또 다시 분노를 일으켰으며, 예술의 전당 디자인미술관 정문에서 현역 사진작가들이 릴레이 1인 시위를 했을 정도다. 일본은 한국과 마찬가지로 형사소송법상의 기소편의주의에 의해 사건에 대한 불기소처분을 검찰관(검사)의 재량에 따라 할 수 있다. 하지만 고소한 사람이 이에 불복하는 경우 민간인으로 이루어진 검찰심사회에 불기소 처분이 타당한지에 대한 심사를 요청할 수 있으며, 이는 민의를 이용한 기소독점주의에 대한 하나의 견제 방편이다.\\\\n\\\\n일본 검찰심사회에서는 2008년까지 약 15만건의 불기소 사건들을 심사해 그 중 11.3%에 대해 \"불기소 부당\" 또는 \"기소 상당\"의 의견으로 의결했고, 검찰은 이 중 1400여건을 기소해 징역 10년형의 중형에 처해진 경우도 있다. 검찰심사회는 미나마타병 사건, 일본항공 350편 하네다해 추락 사건 , 일본항공 123편 점보제트기 추락 사건, 에이즈 혈우병 치료제 사건 , 토요하마 터널 암반 추락 사건 , 유키지루시 집단 식중독 사건 , 아카시 불꽃놀이 보도교 사고 및 오자와 이치로 일본 민주당 간사장의 기소 처분 등의 사회적 이목을 끈 사건들을 맡기도 했다.\\\\n\\\\n일본에서는 검찰심사원으로 연간 약 7,300명이 선발되며, 총 54만명이 심사원으로 일해본 경험이 있다. 현재 미국, 일본, 프랑스에서 도입중이다. 2005년에 포이즌 필을 도입한 일본에서는 분쟁을 통해 판례까지 등장하고 있다 113 대한민국의 경우, 기업들이 포이즌 필, 차등의결권주식 등을 포함한 새로운 경영권 방어 장치의 도입을 계속 요구하였 30 대한민국의 학계에서도 이미 많은 논의가 진행되어 왔다 113 코스닥 등록 창투사인 옵셔널벤처스코리아는 2001년 7월 대표이사가 임기 중 타의에 의하여 강제퇴임할 경우 50억원의 위로금을 지급해야 한다는 포이즌 필 조항을을 채택하하여 첨단 M&A 방어기법이냐, 건전한 M&A를 막는 독소조항이냐라는 논란을 불러일으켰다. 옵셔널벤처스코리아의 2대 주주인 광주은행이 소액주주들에게 의결권위임을 받아 임원퇴직금지급규정 변경안을 저지하려고 하였으나 실패하였다. 현재 법무부가 준비 중인 상법 개정안은 신주인수선택권이라는 명칭으로 포이즌 필을 도입하려고 한다. 이에 의하면 회사는 정관으로 주주에게 그가 가진 주식의 종류 및 수에 따라 미리 정한 가액으로 일정한 기간 내에 회사에 대하여 신주의 발행을 청구할 수 있는 권리를 부여할 수 있으며 이 권리가 신주인수선택권이다 114'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_df['question'][0])\n",
    "train_df['context'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "new_train_dataset = Dataset.from_pandas(train_df)\n",
    "new_train_dataset.save_to_disk('/opt/ml/data/train_dataset/new_validation_v2')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_train_dataset['context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3952/3952 [15:34<00:00,  4.23it/s]\n"
     ]
    }
   ],
   "source": [
    "new_reverse_context = []\n",
    "new_reverse_answer = []\n",
    "# len(train_dataset)\n",
    "for i in tqdm(range(len(train_dataset))):\n",
    "    train_context = train_dataset['context'][i]\n",
    "    sim_context = dense_train_retrieval_result[query[i]] # context list\n",
    "\n",
    "\n",
    "    cnt = 1\n",
    "    sim_context_idx = 2\n",
    "    sim_top_k = [] # 정답 context를 제외한 top_k\n",
    "    temp_answer = {}\n",
    "\n",
    "    add_start_idx = 0\n",
    "    while cnt != 0:\n",
    "        if train_context != sim_context[sim_context_idx]:\n",
    "            sim_top_k.append(sim_context[sim_context_idx])\n",
    "            add_start_idx += len(sim_context[sim_context_idx]) + 1 # 공백까지 포함\n",
    "            cnt -= 1\n",
    "        sim_context_idx += 1\n",
    "    sim_top_k.append(train_context)\n",
    "    add_sim_context = ' '.join(sim_top_k)\n",
    "    # 시간 복잡도?\n",
    "    temp_answer['answer_start'] = [train_dataset['answers'][i]['answer_start'][0] + add_start_idx]\n",
    "    temp_answer['text'] = train_dataset['answers'][i]['text']\n",
    "    new_reverse_answer.append(temp_answer)\n",
    "    new_reverse_context.append(add_sim_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reverse_df = train_dataset.to_pandas()\n",
    "train_reverse_df['context'] = new_reverse_context\n",
    "train_reverse_df['answers'] = new_reverse_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'백'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_reverse_df['context'][2][train_reverse_df['answers'][2]['answer_start'][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3952/3952 [15:44<00:00,  4.18it/s]\n"
     ]
    }
   ],
   "source": [
    "new_reverse_negative_context = []\n",
    "new_reverse_negative_answer = []\n",
    "# len(train_dataset)\n",
    "for i in tqdm(range(len(train_dataset))):\n",
    "    train_context = train_dataset['context'][i]\n",
    "    sim_context = dense_train_retrieval_result[query[i]] # context list\n",
    "\n",
    "\n",
    "    cnt = 1\n",
    "    sim_context_idx = -2\n",
    "    sim_top_k = [] # 정답 context를 제외한 top_k\n",
    "    temp_answer = {}\n",
    "\n",
    "    add_start_idx = 0\n",
    "    while cnt != 0:\n",
    "        if train_context != sim_context[sim_context_idx]:\n",
    "            sim_top_k.append(sim_context[sim_context_idx])\n",
    "            add_start_idx += len(sim_context[sim_context_idx]) + 1 # 공백까지 포함\n",
    "            cnt -= 1\n",
    "        sim_context_idx -= 1\n",
    "    sim_top_k.append(train_context)\n",
    "    add_sim_context = ' '.join(sim_top_k)\n",
    "    # 시간 복잡도?\n",
    "    temp_answer['answer_start'] = [train_dataset['answers'][i]['answer_start'][0] + add_start_idx]\n",
    "    temp_answer['text'] = train_dataset['answers'][i]['text']\n",
    "    new_reverse_negative_answer.append(temp_answer)\n",
    "    new_reverse_negative_context.append(add_sim_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reverse_negative_df = train_dataset.to_pandas()\n",
    "train_reverse_negative_df['context'] = new_reverse_negative_context\n",
    "train_reverse_negative_df['answers'] = new_reverse_negative_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_origin_df = train_dataset.to_pandas()\n",
    "all_train = pd.concat([train_origin_df, train_df, train_negative_df,train_reverse_df,train_reverse_negative_df])\n",
    "all_train.reset_index(drop=True)\n",
    "all_train = all_train.drop(['__index_level_0__'],axis=1)\n",
    "new_train_dataset = Dataset.from_pandas(all_train)\n",
    "new_train_dataset.save_to_disk('/opt/ml/data/train_dataset/new_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train = all_train.drop(['__index_level_0__'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_dataset = Dataset.from_pandas(all_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
