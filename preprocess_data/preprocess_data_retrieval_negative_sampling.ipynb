{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "from rank_bm25 import BM25Okapi\n",
    "import pandas as pd\n",
    "from datasets import load_from_disk, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_dataset = pd.read_json('../data/wikipedia_documents.json',orient='index')\n",
    "train_dataset = load_from_disk('/opt/ml/data/train_dataset/train').to_pandas()\n",
    "valid_dataset = load_from_disk('/opt/ml/data/train_dataset/validation').to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    text = re.sub(r'\\\\n', ' ', text) # remove newline character\n",
    "    text = re.sub(r'\\s+', ' ', text) # remove continuous spaces\n",
    "    text = re.sub(r'#', ' ', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_dataset['text'] = wiki_dataset['text'].apply(preprocess)\n",
    "#train_dataset['context'] = train_dataset['context'].apply(preprocess)\n",
    "#valid_dataset['context'] = valid_dataset['context'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_corpus = list(set([example_wiki for example_wiki in wiki_dataset['text']]))\n",
    "train_context = train_dataset['context']\n",
    "valid_context = valid_dataset['context']\n",
    "train_query = train_dataset['question']\n",
    "valid_query = valid_dataset['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (945 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')\n",
    "tokenized_corpus = [tokenizer.tokenize(context) for context in wiki_corpus]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "INDEX_NAME = \"wiki_index\"\n",
    "INDEX_SETTINGS = {\n",
    "  \"settings\" : {\n",
    "    \"index\":{\n",
    "      \"analysis\":{\n",
    "        \"analyzer\":{\n",
    "          \"korean\":{\n",
    "            \"type\":\"custom\",\n",
    "            \"tokenizer\":\"nori_tokenizer\",\n",
    "            \"filter\": [ \"shingle\" ],\n",
    "\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"mappings\": {\n",
    "\n",
    "      \"properties\" : {\n",
    "        \"content\" : {\n",
    "          \"type\" : \"text\",\n",
    "          \"analyzer\": \"korean\",\n",
    "          \"search_analyzer\": \"korean\"\n",
    "        },\n",
    "        \"title\" : {\n",
    "          \"type\" : \"text\",\n",
    "          \"analyzer\": \"korean\",\n",
    "          \"search_analyzer\": \"korean\"\n",
    "        }\n",
    "      }\n",
    "\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    {\n",
    "        '_index' : INDEX_NAME,\n",
    "        '_id' : wiki_dataset.iloc[i]['document_id'],\n",
    "        'title' : wiki_dataset.iloc[i]['title'],\n",
    "        'content' : wiki_dataset.iloc[i]['text']\n",
    "    }\n",
    "    for i in range(wiki_dataset.shape[0])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    es.transport.close()\n",
    "except:\n",
    "    pass\n",
    "es = Elasticsearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if es.indices.exists(INDEX_NAME):\n",
    "    es.indices.delete(index=INDEX_NAME)\n",
    "es.indices.create(index=INDEX_NAME, body=INDEX_SETTINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESPONSE: (60613, [])\n",
      "106.88828921318054\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "try:\n",
    "    response = helpers.bulk(es, docs)\n",
    "    print (\"\\nRESPONSE:\", response)\n",
    "except Exception as e:\n",
    "    print(\"\\nERROR:\", e)\n",
    "    \n",
    "end_time = time.time()\n",
    "\n",
    "print(end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/elasticsearch/connection/base.py:209: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.15/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    }
   ],
   "source": [
    "res = es.search(index=INDEX_NAME, q=train_query[1], size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3952/3952 [03:27<00:00, 19.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# reader_data\n",
    "\n",
    "train_data_with_elastic = {}\n",
    "error_questions = []\n",
    "for i in tqdm(range(len(train_query))):\n",
    "    try:\n",
    "        res = es.search(index=INDEX_NAME, q=train_query[i], size=100)\n",
    "        hits = res['hits']['hits']\n",
    "\n",
    "        top_k_list = []\n",
    "        for hit in hits:\n",
    "            ctx = hit['_source']['content']\n",
    "            top_k_list.append(ctx)\n",
    "        train_data_with_elastic[train_query[i]] = top_k_list\n",
    "    except Exception as e:\n",
    "        error_questions.append([e, train_query[i]])\n",
    "        train_data_with_elastic[train_query[i]] = bm25.get_top_n(tokenizer.tokenize(train_query[i]), wiki_corpus, n=100)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240/240 [00:13<00:00, 17.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# reader_data\n",
    "valid_data_with_elastic = {}\n",
    "error_valid_questions = []\n",
    "for i in tqdm(range(len(valid_query))):\n",
    "    try:\n",
    "        res = es.search(index=INDEX_NAME, q=valid_query[i], size=100)\n",
    "        hits = res['hits']['hits']\n",
    "\n",
    "        top_k_list = []\n",
    "        for hit in hits:\n",
    "            ctx = hit['_source']['content']\n",
    "            top_k_list.append(ctx)\n",
    "        valid_data_with_elastic[valid_query[i]] = top_k_list\n",
    "    except Exception as e:\n",
    "        error_valid_questions.append([e, valid_query[i]])\n",
    "        valid_data_with_elastic[valid_query[i]] = bm25.get_top_n(tokenizer.tokenize(valid_query[i]), wiki_corpus, n=100)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/opt/ml/data/elastic_train_100.bin', \"wb\") as file:\n",
    "    pickle.dump(train_data_with_elastic, file)\n",
    "with open('/opt/ml/data/elastic_valid_100.bin', \"wb\") as file:\n",
    "    pickle.dump(valid_data_with_elastic, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = load_from_disk('/opt/ml/data/test_dataset/validation').to_pandas()\n",
    "test_query = test_dataset['question']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:32<00:00, 18.60it/s]\n"
     ]
    }
   ],
   "source": [
    "test_data_with_elastic = {}\n",
    "error_test_questions = []\n",
    "for i in tqdm(range(len(test_query))):\n",
    "    try:\n",
    "        res = es.search(index=INDEX_NAME, q=test_query[i], size=100)\n",
    "        hits = res['hits']['hits']\n",
    "\n",
    "        top_k_list = []\n",
    "        for hit in hits:\n",
    "            ctx = hit['_source']['content']\n",
    "            top_k_list.append(ctx)\n",
    "        test_data_with_elastic[test_query[i]] = top_k_list\n",
    "    except Exception as e:\n",
    "        error_test_questions.append([e, test_query[i]])\n",
    "        test_data_with_elastic[test_query[i]] = bm25.get_top_n(tokenizer.tokenize(test_query[i]), wiki_corpus, n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/opt/ml/data/elastic_retrieval.bin', \"wb\") as file:\n",
    "    pickle.dump(test_data_with_elastic, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/opt/ml/data/elastic_train_100.bin', \"rb\") as file:\n",
    "    train_data_with_elastic = pickle.load(file)\n",
    "with open('/opt/ml/data/elastic_valid_100.bin', \"rb\") as file:\n",
    "    valid_data_with_elastic = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대통령을 포함한 미국의 행정부 견제권을 갖는 국가 기관은?\n",
      "['미국 상의원 또는 미국 상원(United States Senate)은 양원제인 미국 의회의 상원이다. 미국 부통령이 상원의장이 된다. 각 주당 2명의 상원의원이 선출되어 100명의 상원의원으로 구성되어 있다. 임기는 6년이며, 2년마다 50개주 중 1/3씩 상원의원을 새로 선출하여 연방에 보낸다. 미국 상원은 미국 하원과는 다르게 미국 대통령을 수반으로 하는 미국 연방 행정부에 각종 동의를 하는 기관이다. 하원이 세금과 경제에 대한 권한, 대통령을 포함한 대다수의 공무원을 파면할 권한을 갖고 있는 국민을 대표하는 기관인 반면 상원은 미국의 주를 대표한다. 즉 캘리포니아주, 일리노이주 같이 주 정부와 주 의회를 대표하는 기관이다. 그로 인하여 군대의 파병, 관료의 임명에 대한 동의, 외국 조약에 대한 승인 등 신속을 요하는 권한은 모두 상원에게만 있다. 그리고 하원에 대한 견제 역할(하원의 법안을 거부할 권한 등)을 담당한다. 2년의 임기로 인하여 급진적일 수밖에 없는 하원은 지나치게 급진적인 법안을 만들기 쉽다. 대표적인 예로 건강보험 개혁 당시 하원이 미국 연방 행정부에게 퍼블릭 옵션(공공건강보험기관)의 조항이 있는 반면 상원의 경우 하원안이 지나치게 세금이 많이 든다는 이유로 퍼블릭 옵션 조항을 제외하고 비영리건강보험기관이나 보험회사가 담당하도록 한 것이다. 이 경우처럼 상원은 하원이나 내각책임제가 빠지기 쉬운 국가들의 국회처럼 걸핏하면 발생하는 의회의 비정상적인 사태를 방지하는 기관이다. 상원은 급박한 처리사항의 경우가 아니면 법안을 먼저 내는 경우가 드물고 하원이 만든 법안을 수정하여 다시 하원에 되돌려보낸다. 이러한 방식으로 단원제가 빠지기 쉬운 함정을 미리 방지하는 것이다.날짜=2017-02-05', '내각(內閣, cabinet)은 행정부의 주요 각료들로 구성되는 국가의 주요기관이다. 의원내각제에서 내각은 수상과 여러 장관으로 조직되는 합의체로, 국가의 행정권을 담당하고 국회에 대한 연대책임을 갖는다. 의원내각제에 있어서 내각은 국가행정의 최고기관인 한편 국민이 구성시키는 의회에 의하여 철저히 견제되어 의회민주주의 체제를 이룬다. 그 직접적 유래는 영국에서 국왕의 정치를 자문하던 추밀원에서 찾을 수 있다. 특히 내각은 추밀원의 일개 회의에서 시작하였다가 권한이 집중되어 분리된 기관으로, 이후 국왕의 실권이 사라지고 일명 웨스트민스터 시스템으로 불리는 의원내각제가 성립하면서 의회에 의한 민주적 행정부를 이루게 되었다. 한편 국가원수에게 대부분의 권력이 집중되는 대통령중심제와 군주제에서 내각은 원칙적으로 의결권이 없거나 의결의 구속력이 없는 보좌기관에 불과한 경우가 많다.(예: 대한민국의 국무회의) 대한민국은 국무회의가 내각에 속하며 권한이 대통령에 비해 제한적이다. 과거 왕조시대 때는 고려시대의 중서문하성, 중추원, 육부 또는 조선시대의 의정부와 육조가 내각과 비슷한 성향을 지니고 있었다.']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_query)):\n",
    "    print(train_query[i])\n",
    "    print(train_data_with_elastic[train_query[i]][:2])\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
