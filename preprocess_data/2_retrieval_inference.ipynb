{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from retrieval_model import BertEncoder,RobertaEncoder\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import (DataLoader,TensorDataset, SequentialSampler)\n",
    "import pickle\n",
    "from utils_mrc import seed_everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DPR model load\n",
    "p_encoder = BertEncoder.from_pretrained(\"/opt/ml/mrc-level2-nlp-08/retrieval/p_encoder\")\n",
    "q_encoder = BertEncoder.from_pretrained(\"/opt/ml/mrc-level2-nlp-08/retrieval/q_encoder\")\n",
    "tokenizer =  AutoTokenizer.from_pretrained(\"kykim/bert-kor-base\")\n",
    "#\"kykim/bert-kor-base\"\n",
    "# \"monologg/kobigbird-bert-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset = load_from_disk(\"/opt/ml/data/train_dataset/train/\")\n",
    "#query_dataset = load_from_disk('/opt/ml/data/test_dataset/validation') # test query\n",
    "#train_dataset = load_from_disk(\"/opt/ml/data/train_dataset/new_validation/\")\n",
    "#origin_valid = load_from_disk(\"/opt/ml/data/train_dataset/validation/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wiki_dataset = pd.read_csv('/opt/ml/data/preprocess_wiki_doc.csv')\n",
    "query_dataset = load_from_disk(\"/opt/ml/data/train_dataset/validation/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/opt/ml/data/wiki_id_context_pair.bin\", \"rb\") as f:\n",
    "    wiki_id_context = pickle.load(f)\n",
    "# context - doc_id\n",
    "with open(\"/opt/ml/data/wiki_context_id_pair.bin\", \"rb\") as f:\n",
    "    wiki_context_id = pickle.load(f)\n",
    "with open(\"/opt/ml/data/wiki_id_title_pair.bin\", \"rb\") as f:\n",
    "    wiki_id_title = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_corpus = list(wiki_context_id.keys())\n",
    "wiki_title_corpus = []\n",
    "for i in range(len(wiki_corpus)):\n",
    "    wiki_title_corpus.append(wiki_id_title[wiki_context_id[wiki_corpus[i]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55963\n",
      "55963\n"
     ]
    }
   ],
   "source": [
    "print(len(wiki_corpus))\n",
    "print(len(wiki_title_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = query_dataset['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 1749/1749 [09:44<00:00,  2.99it/s]\n"
     ]
    }
   ],
   "source": [
    "eval_batch_size = 32\n",
    "def to_cuda(batch):\n",
    "  return tuple(t.cuda() for t in batch)\n",
    "if torch.cuda.is_available():\n",
    "    p_encoder.cuda()\n",
    "    q_encoder.cuda()\n",
    "\n",
    "# Construt dataloader\n",
    "#train_p_seqs = tokenizer(wiki_title_corpus,wiki_corpus, max_length=512, padding=\"max_length\", truncation=True, return_tensors='pt') # add title\n",
    "train_p_seqs = tokenizer(wiki_corpus, max_length=512, padding=\"max_length\", truncation=True, return_tensors='pt') # no title\n",
    "\n",
    "valid_dataset = TensorDataset(\n",
    "    train_p_seqs[\"input_ids\"],\n",
    "    train_p_seqs[\"attention_mask\"],\n",
    "    train_p_seqs[\"token_type_ids\"]\n",
    ")\n",
    "valid_sampler = SequentialSampler(valid_dataset)\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    sampler=valid_sampler,\n",
    "    batch_size=eval_batch_size\n",
    ")\n",
    "\n",
    "# Inference using the passage encoder to get dense embeddeings\n",
    "p_embs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    epoch_iterator = tqdm(\n",
    "        valid_dataloader,\n",
    "        desc=\"Iteration\",\n",
    "        position=0,\n",
    "        leave=True\n",
    "    )\n",
    "    p_encoder.eval()\n",
    "\n",
    "    for _, batch in enumerate(epoch_iterator):\n",
    "        batch = tuple(t.cuda() for t in batch)\n",
    "\n",
    "        p_inputs = {\n",
    "            \"input_ids\": batch[0],\n",
    "            \"attention_mask\": batch[1],\n",
    "            \"token_type_ids\": batch[2]\n",
    "        }\n",
    "        \n",
    "        outputs = p_encoder(**p_inputs).to(\"cpu\").numpy()\n",
    "        p_embs.extend(outputs)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 8/8 [00:00<00:00, 25.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_q_seqs = tokenizer(\n",
    "    query,\n",
    "    max_length=64,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "query_dataset = TensorDataset(\n",
    "    train_q_seqs[\"input_ids\"],\n",
    "    train_q_seqs[\"attention_mask\"],\n",
    "    train_q_seqs[\"token_type_ids\"]\n",
    ")\n",
    "\n",
    "query_sampler = SequentialSampler(query_dataset)\n",
    "query_dataloader = DataLoader(\n",
    "    query_dataset,\n",
    "    sampler=query_sampler,\n",
    "    batch_size=eval_batch_size\n",
    ")\n",
    "\n",
    "q_embs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    epoch_iterator = tqdm(\n",
    "        query_dataloader,\n",
    "        desc=\"Iteration\",\n",
    "        position=0,\n",
    "        leave=True\n",
    "    )\n",
    "    q_encoder.eval()\n",
    "\n",
    "    for _, batch in enumerate(epoch_iterator):\n",
    "        batch = tuple(t.cuda() for t in batch)\n",
    "\n",
    "        q_inputs = {\n",
    "            \"input_ids\": batch[0],\n",
    "            \"attention_mask\": batch[1],\n",
    "            \"token_type_ids\": batch[2]\n",
    "        }\n",
    "        \n",
    "        outputs = q_encoder(**q_inputs).to(\"cpu\").numpy()\n",
    "        q_embs.extend(outputs)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55963, 768)\n",
      "(240, 768)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nvalid-query1 - [{id : score}....]\\nvalid-query1 - [{id : score}....]\\nvalid-query1 - [{id : score}....]\\n\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_embs = np.array(p_embs)\n",
    "q_embs = np.array(q_embs)\n",
    "print(p_embs.shape)\n",
    "print(q_embs.shape)\n",
    "'''\n",
    "valid-query1 - [{id : score}....]\n",
    "valid-query1 - [{id : score}....]\n",
    "valid-query1 - [{id : score}....]\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/opt/ml/data/dense_embedding.bin\", \"wb\") as file:\n",
    "    pickle.dump(p_embs,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    p_embs_cuda = torch.Tensor(p_embs).to('cuda')\n",
    "    q_embs_cuda = torch.Tensor(q_embs).to('cuda')\n",
    "\n",
    "dot_prod_scores = torch.matmul(q_embs_cuda, torch.transpose(p_embs_cuda, 0, 1))\n",
    "rank = torch.argsort(dot_prod_scores, dim=1, descending=True).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240/240 [00:00<00:00, 481.24it/s]\n"
     ]
    }
   ],
   "source": [
    "dense_p_retrieval_result = {}\n",
    "idx = 0\n",
    "for i in tqdm(range(len(query))):\n",
    "    p_list = []\n",
    "    q = query[i]\n",
    "    for j in range(100):\n",
    "        p_list.append(wiki_context_id[wiki_corpus[rank[idx][j]]])\n",
    "    dense_p_retrieval_result[q] = p_list\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/opt/ml/data/dense_valid_retrieval.bin\", \"wb\") as file:\n",
    "    pickle.dump(dense_p_retrieval_result,file)\n",
    "\n",
    "# dense_n_retrieval_result = {}\n",
    "# idx = 0\n",
    "# for i in tqdm(range(len(query))):\n",
    "#     p_list = []\n",
    "#     q = query[i]\n",
    "#     for j in range(10000,10004):\n",
    "#         p_list.append(wiki_corpus[rank[idx][j]])\n",
    "#     dense_n_retrieval_result[q] = p_list\n",
    "#     idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_top_k :  1\n",
      "elastic ACC :  0.7166666666666667\n",
      "Dense ACC :  0.375\n",
      "\n",
      "score_top_k :  5\n",
      "elastic ACC :  0.8625\n",
      "Dense ACC :  0.6208333333333333\n",
      "\n",
      "score_top_k :  10\n",
      "elastic ACC :  0.9125\n",
      "Dense ACC :  0.6541666666666667\n",
      "\n",
      "score_top_k :  15\n",
      "elastic ACC :  0.925\n",
      "Dense ACC :  0.6958333333333333\n",
      "\n",
      "score_top_k :  20\n",
      "elastic ACC :  0.9375\n",
      "Dense ACC :  0.7041666666666667\n",
      "\n",
      "score_top_k :  25\n",
      "elastic ACC :  0.9416666666666667\n",
      "Dense ACC :  0.725\n",
      "\n",
      "score_top_k :  30\n",
      "elastic ACC :  0.95\n",
      "Dense ACC :  0.7333333333333333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "valid_dataset = load_from_disk(\"/opt/ml/data/new_train_dataset/validation\")\n",
    "with open('/opt/ml/data/wiki_context_id_pair.bin','rb') as f:\n",
    "    wiki_context_id = pickle.load(f)\n",
    "with open('/opt/ml/data/wiki_id_context_pair.bin','rb') as f:\n",
    "    wiki_id_context = pickle.load(f)\n",
    "with open('/opt/ml/data/elastic_valid_1000.bin','rb') as f:\n",
    "    elastic_valid = pickle.load(f)\n",
    "\n",
    "dense_valid = dense_p_retrieval_result\n",
    "query = valid_dataset['question']\n",
    "context = valid_dataset['context']\n",
    "\n",
    "top_k_list = [1,5,10,15,20,25,30]\n",
    "\n",
    "for top_k in top_k_list:\n",
    "    elastic_acc = 0\n",
    "    dense_acc = 0\n",
    "    for i in range(len(query)):\n",
    "        q = query[i]\n",
    "        ground_truth = context[i]\n",
    "        dense_top_k = []\n",
    "        for j in range(top_k):\n",
    "            dense_top_k.append(wiki_id_context[dense_valid[q][j]])\n",
    "\n",
    "        elastic_top_k = []\n",
    "        for j in range(top_k):\n",
    "            elastic_top_k.append(wiki_id_context[elastic_valid[q][j]])\n",
    "            \n",
    "        if ground_truth in elastic_top_k:\n",
    "            elastic_acc += 1\n",
    "        if ground_truth in dense_top_k:\n",
    "            dense_acc += 1\n",
    "\n",
    "    print('score_top_k : ', top_k)\n",
    "    print('elastic ACC : ', elastic_acc / len(query))\n",
    "    print('Dense ACC : ', dense_acc / len(query))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "score_top_k :  1\n",
    "elastic ACC :  0.7166666666666667\n",
    "Dense ACC :  0.375\n",
    "\n",
    "score_top_k :  5\n",
    "elastic ACC :  0.8625\n",
    "Dense ACC :  0.5875\n",
    "\n",
    "score_top_k :  10\n",
    "elastic ACC :  0.9125\n",
    "Dense ACC :  0.6541666666666667\n",
    "\n",
    "score_top_k :  15\n",
    "elastic ACC :  0.925\n",
    "Dense ACC :  0.7\n",
    "\n",
    "score_top_k :  20\n",
    "elastic ACC :  0.9375\n",
    "Dense ACC :  0.7166666666666667\n",
    "\n",
    "score_top_k :  25\n",
    "elastic ACC :  0.9416666666666667\n",
    "Dense ACC :  0.7375\n",
    "\n",
    "score_top_k :  30\n",
    "elastic ACC :  0.95\n",
    "Dense ACC :  0.7625\n",
    "'''\n",
    "'''\n",
    "score_top_k :  1\n",
    "elastic ACC :  0.7166666666666667\n",
    "Dense ACC :  0.375\n",
    "\n",
    "score_top_k :  5\n",
    "elastic ACC :  0.8625\n",
    "Dense ACC :  0.5875\n",
    "\n",
    "score_top_k :  10\n",
    "elastic ACC :  0.9125\n",
    "Dense ACC :  0.6541666666666667\n",
    "\n",
    "score_top_k :  15\n",
    "elastic ACC :  0.925\n",
    "Dense ACC :  0.7\n",
    "\n",
    "score_top_k :  20\n",
    "elastic ACC :  0.9375\n",
    "Dense ACC :  0.7166666666666667\n",
    "\n",
    "score_top_k :  25\n",
    "elastic ACC :  0.9416666666666667\n",
    "Dense ACC :  0.7375\n",
    "\n",
    "score_top_k :  30\n",
    "elastic ACC :  0.95\n",
    "Dense ACC :  0.7625\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
