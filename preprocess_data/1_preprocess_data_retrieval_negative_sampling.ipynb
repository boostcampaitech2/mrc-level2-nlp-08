{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "from rank_bm25 import BM25Okapi\n",
    "import pandas as pd\n",
    "from datasets import load_from_disk, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import re\n",
    "from pororo import Pororo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!service elasticsearch start\n",
    "#!service elasticsearch stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elastic+  5929     1  1 Nov01 ?        00:19:37 /usr/share/elasticsearch/jdk/bin/java -Xshare:auto -Des.networkaddress.cache.ttl=60 -Des.networkaddress.cache.negative.ttl=10 -XX:+AlwaysPreTouch -Xss1m -Djava.awt.headless=true -Dfile.encoding=UTF-8 -Djna.nosys=true -XX:-OmitStackTraceInFastThrow -XX:+ShowCodeDetailsInExceptionMessages -Dio.netty.noUnsafe=true -Dio.netty.noKeySetOptimization=true -Dio.netty.recycler.maxCapacityPerThread=0 -Dio.netty.allocator.numDirectArenas=0 -Dlog4j.shutdownHookEnabled=false -Dlog4j2.disable.jmx=true -Djava.locale.providers=SPI,COMPAT --add-opens=java.base/java.io=ALL-UNNAMED -XX:+UseG1GC -Djava.io.tmpdir=/tmp/elasticsearch-5192679610809952812 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/var/lib/elasticsearch -XX:ErrorFile=/var/log/elasticsearch/hs_err_pid%p.log -Xlog:gc*,gc+age=trace,safepoint:file=/var/log/elasticsearch/gc.log:utctime,pid,tags:filecount=32,filesize=64m -Xms31744m -Xmx31744m -XX:MaxDirectMemorySize=16642998272 -XX:InitiatingHeapOccupancyPercent=30 -XX:G1ReservePercent=25 -Des.path.home=/usr/share/elasticsearch -Des.path.conf=/etc/elasticsearch -Des.distribution.flavor=default -Des.distribution.type=deb -Des.bundled_jdk=true -cp /usr/share/elasticsearch/lib/* org.elasticsearch.bootstrap.Elasticsearch -d -p /var/run/elasticsearch/elasticsearch.pid\n",
      "elastic+  5979  5929  0 Nov01 ?        00:00:00 /usr/share/elasticsearch/modules/x-pack-ml/platform/linux-x86_64/bin/controller\n",
      "root     29316 29294 63 03:16 pts/5    00:00:00 /bin/bash -c ps -ef | grep elastic\n",
      "root     29318 29316  0 03:16 pts/5    00:00:00 /bin/bash -c ps -ef | grep elastic\n"
     ]
    }
   ],
   "source": [
    "!ps -ef | grep elastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_dataset = pd.read_json('/opt/ml/data/preprocess_wiki.json',orient='index') # 전처리된 위키\n",
    "wiki_dataset = wiki_dataset.drop_duplicates(['text','title'],ignore_index=True) # 중복 제거\n",
    "\n",
    "train_dataset = load_from_disk('/opt/ml/data/train_dataset/train').to_pandas()\n",
    "valid_dataset = load_from_disk('/opt/ml/data/train_dataset/validation').to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ctx_front = []\n",
    "# ctx_rear = []\n",
    "# for i in range(len(wiki_dataset)):\n",
    "#     ctx_len = len(wiki_dataset['text'][i])\n",
    "#     ctx_front.append(wiki_dataset['text'][i][:ctx_len // 2])\n",
    "#     ctx_rear.append(wiki_dataset['text'][i][ctx_len // 2:])\n",
    "\n",
    "# temp1 = wiki_dataset[wiki_dataset.columns]\n",
    "# temp2 = wiki_dataset[wiki_dataset.columns]\n",
    "\n",
    "# temp1['text'] = ctx_front\n",
    "# temp2['text'] = ctx_rear\n",
    "\n",
    "# total = pd.concat([temp1,temp2],ignore_index=True)\n",
    "# wiki_dataset = total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ctx_rear[0])\n",
    "#print(ctx_front[0])\n",
    "#total['text'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/opt/ml/data/wiki_context_id_pair.bin','rb') as f:\n",
    "    wiki_context_id_pair = pickle.load(f)\n",
    "\n",
    "with open('/opt/ml/data/wiki_id_context_pair.bin','rb') as f:\n",
    "    wiki_id_context_pair = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wiki_corpus = list(set([example_wiki for example_wiki in wiki_dataset['text']]))\n",
    "train_context = train_dataset['context']\n",
    "valid_context = valid_dataset['context']\n",
    "train_query = train_dataset['question']\n",
    "valid_query = valid_dataset['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1131 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "wiki_corpus = list(wiki_context_id_pair.keys())\n",
    "tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')\n",
    "tokenized_corpus = [tokenizer.tokenize(context) for context in wiki_corpus]\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_NAME = \"wiki_index\"\n",
    "INDEX_SETTINGS = {\n",
    "    \"settings\": {\n",
    "        \"index\": {\n",
    "            \"analysis\": {\n",
    "                \"analyzer\": {\n",
    "                    \"korean\": {\n",
    "                        \"type\": \"custom\",\n",
    "                        \"tokenizer\": \"nori_tokenizer\",\n",
    "                        \"filter\": [\"shingle\"],\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"content\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"korean\",\n",
    "                \"search_analyzer\": \"korean\",\n",
    "            },\n",
    "            \"title\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"korean\",\n",
    "                \"search_analyzer\": \"korean\",\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "}\n",
    "# INDEX_SETTINGS = {\n",
    "#     \"settings\": {\n",
    "#         \"analysis\": {\n",
    "#             \"analyzer\": {\n",
    "#                 \"my_analyzer\": {\n",
    "#                     \"type\": \"custom\",\n",
    "#                     \"tokenizer\": \"nori_tokenizer\",\n",
    "#                     \"decompound_mode\": \"mixed\",\n",
    "#                     \"stopwords\": \"_korean_\",\n",
    "#                     \"synonyms\": \"_korean_\",\n",
    "#                     \"filter\": [\n",
    "#                         \"lowercase\",\n",
    "#                         \"my_shingle_f\",\n",
    "#                         \"nori_readingform\",\n",
    "#                         \"nori_number\",\n",
    "#                         \"cjk_bigram\",\n",
    "#                         \"decimal_digit\",\n",
    "#                         \"stemmer\",\n",
    "#                         \"trim\",\n",
    "#                     ],\n",
    "#                 }\n",
    "#             },\n",
    "#             \"filter\": {\"my_shingle_f\": {\"type\": \"shingle\"}},\n",
    "#         },\n",
    "#         \"similarity\": {\n",
    "#             \"my_similarity\": {\n",
    "#                 \"type\": \"BM25\",\n",
    "#             }\n",
    "#         },\n",
    "#     },\n",
    "#     \"mappings\": {\n",
    "#         \"properties\": {\n",
    "#             \"title\": {\n",
    "#                 \"type\": \"text\",\n",
    "#                 \"analyzer\": \"my_analyzer\",\n",
    "#                 \"similarity\": \"my_similarity\",\n",
    "#             },\n",
    "#             \"text\": {\n",
    "#                 \"type\": \"text\",\n",
    "#                 \"analyzer\": \"my_analyzer\",\n",
    "#                 \"similarity\": \"my_similarity\",\n",
    "#             },\n",
    "#             \"text_origin\": {\n",
    "#                 \"type\": \"text\",\n",
    "#                 \"analyzer\": \"my_analyzer\",\n",
    "#                 \"similarity\": \"my_similarity\",\n",
    "#             },\n",
    "#         }\n",
    "#     },\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    {\n",
    "        '_index' : INDEX_NAME,\n",
    "        '_id' : wiki_dataset.iloc[i]['document_id'],\n",
    "        'title' : wiki_dataset.iloc[i]['title'],\n",
    "        'content' : wiki_dataset.iloc[i]['text']\n",
    "    }\n",
    "    for i in range(wiki_dataset.shape[0])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ps -ef | grep elastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    es.transport.close()\n",
    "except:\n",
    "    pass\n",
    "#config = {'host':'localhost', 'port':2293}\n",
    "es = Elasticsearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': '89aa4310917b',\n",
       " 'cluster_name': 'elasticsearch',\n",
       " 'cluster_uuid': 'tWsEBuE7ReuuGa-pZkywtw',\n",
       " 'version': {'number': '7.15.1',\n",
       "  'build_flavor': 'default',\n",
       "  'build_type': 'deb',\n",
       "  'build_hash': '83c34f456ae29d60e94d886e455e6a3409bba9ed',\n",
       "  'build_date': '2021-10-07T21:56:19.031608185Z',\n",
       "  'build_snapshot': False,\n",
       "  'lucene_version': '8.9.0',\n",
       "  'minimum_wire_compatibility_version': '6.8.0',\n",
       "  'minimum_index_compatibility_version': '6.0.0-beta1'},\n",
       " 'tagline': 'You Know, for Search'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-b31c6028f9a8>:1: DeprecationWarning: Using positional arguments for APIs is deprecated and will be disabled in 8.0.0. Instead use only keyword arguments for all APIs. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  if es.indices.exists(INDEX_NAME):\n",
      "<ipython-input-24-b31c6028f9a8>:3: DeprecationWarning: The 'body' parameter is deprecated for the 'create' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  es.indices.create(index=INDEX_NAME, body=INDEX_SETTINGS)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'wiki_index'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if es.indices.exists(INDEX_NAME):\n",
    "    es.indices.delete(index=INDEX_NAME)\n",
    "es.indices.create(index=INDEX_NAME, body=INDEX_SETTINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESPONSE: (56044, [])\n",
      "102.2926025390625\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "try:\n",
    "    response = helpers.bulk(es, docs)\n",
    "    print (\"\\nRESPONSE:\", response)\n",
    "except Exception as e:\n",
    "    print(\"\\nERROR:\", e)\n",
    "    \n",
    "end_time = time.time()\n",
    "\n",
    "print(end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    res = es.search(index=INDEX_NAME, q=train_query[0], size=10)\n",
    "except:\n",
    "    res = es.search(index=INDEX_NAME, q=train_query[0], size=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reader_data\n",
    "# train_data_with_elastic = {}\n",
    "# train_data_with_elastic_score = {}\n",
    "\n",
    "# error_questions = []\n",
    "# top_k_num = 300\n",
    "\n",
    "# for i in tqdm(range(len(train_query))):\n",
    "#     try:\n",
    "#         res = es.search(index=INDEX_NAME, q=train_query[i], size=top_k_num)\n",
    "#         hits = res['hits']['hits']\n",
    "        \n",
    "#         top_k_list = []\n",
    "#         top_k_score = []\n",
    "        \n",
    "#         for hit in hits:\n",
    "\n",
    "#             #wiki_id = hit['_id']\n",
    "#             ctx = hit['_source']['content']\n",
    "#             score = hit['_score']\n",
    "            \n",
    "#             top_k_list.append(wiki_context_id_pair[ctx])\n",
    "#             top_k_score.append(score)\n",
    "        \n",
    "#         train_data_with_elastic[train_query[i]] = top_k_list\n",
    "#         train_data_with_elastic_score[train_query[i]] = top_k_score\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         error_questions.append([e, train_query[i]])\n",
    "        \n",
    "#         top_n_id_text = []\n",
    "#         top_n_text = bm25.get_top_n(tokenizer.tokenize(train_query[i]), wiki_corpus, n=top_k_num)\n",
    "        \n",
    "#         for ctx in top_n_text:\n",
    "#             #wiki_id = wiki_dataset['document_id'][wiki_dataset['text'] == ctx]\n",
    "#             top_n_id_text.append(wiki_context_id_pair[ctx])\n",
    "#         #train_data_with_elastic[train_query[i]] = bm25.get_top_n(tokenizer.tokenize(train_query[i]), wiki_corpus, n=top_k_num)\n",
    "        \n",
    "#         train_data_with_elastic[train_query[i]] = top_n_id_text\n",
    "#         temp_score = bm25.get_scores(tokenizer.tokenize(train_query[i])).tolist()\n",
    "#         temp_score.sort(reverse=True)\n",
    "#         train_data_with_elastic_score[train_query[i]] = temp_score[:top_k_num]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 21/240 [00:02<00:24,  9.03it/s][Korean Sentence Splitter]: POST http://localhost:9200/wiki_index/_search?q=%EB%B3%91%EC%97%90+%EA%B1%B8%EB%A0%A4+%EC%A3%BD%EC%9D%84+%ED%99%95%EB%A5%A0%EC%9D%B4+%EC%95%BD+25~50%25%EC%97%90+%EB%8B%AC%ED%95%98%EB%8A%94+%EC%9C%A0%ED%98%95%EC%9D%98+%EC%A7%88%EB%B3%91%EC%9D%80%3F [status:400 request:0.003s]\n",
      "100%|██████████| 240/240 [00:28<00:00,  8.52it/s]\n"
     ]
    }
   ],
   "source": [
    "# reader_data\n",
    "valid_data_with_elastic = {}\n",
    "valid_data_with_elastic_score = {}\n",
    "error_questions = []\n",
    "top_k_num = 300\n",
    "for i in tqdm(range(len(valid_query))):\n",
    "    try:\n",
    "        res = es.search(index=INDEX_NAME, q=valid_query[i], size=top_k_num)\n",
    "        hits = res['hits']['hits']\n",
    "        \n",
    "        top_k_list = []\n",
    "        top_k_score = []\n",
    "        \n",
    "        for hit in hits:\n",
    "\n",
    "            wiki_id = hit['_id']\n",
    "            ctx = hit['_source']['content']\n",
    "            score = hit['_score']\n",
    "            \n",
    "            top_k_list.append(wiki_context_id_pair[ctx])\n",
    "            top_k_score.append(score)\n",
    "        \n",
    "        valid_data_with_elastic[valid_query[i]] = top_k_list\n",
    "        valid_data_with_elastic_score[valid_query[i]] = top_k_score\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_questions.append([e, valid_query[i]])\n",
    "        \n",
    "        top_n_id_text = []\n",
    "        top_n_text = bm25.get_top_n(tokenizer.tokenize(valid_query[i]), wiki_corpus, n=top_k_num)\n",
    "        \n",
    "        for ctx in top_n_text:\n",
    "            #wiki_id = wiki_dataset['document_id'][wiki_dataset['text'] == ctx]\n",
    "            top_n_id_text.append(wiki_context_id_pair[ctx])\n",
    "        #train_data_with_elastic[valid_query[i]] = bm25.get_top_n(tokenizer.tokenize(valid_query[i]), wiki_corpus, n=top_k_num)\n",
    "        \n",
    "        valid_data_with_elastic[valid_query[i]] = top_n_id_text\n",
    "        temp_score = bm25.get_scores(tokenizer.tokenize(valid_query[i])).tolist()\n",
    "        temp_score.sort(reverse=True)\n",
    "        valid_data_with_elastic_score[valid_query[i]] = temp_score[:top_k_num]\n",
    "\n",
    "# 반으로 나누었을때\n",
    "# valid_data_with_elastic = {}\n",
    "# valid_data_with_elastic_score = {}\n",
    "# error_questions = []\n",
    "# top_k_num = 300\n",
    "# for i in tqdm(range(len(valid_query))):\n",
    "#     try:\n",
    "\n",
    "#         res = es.search(index=INDEX_NAME, q=query, size=top_k_num)\n",
    "#         hits = res['hits']['hits']\n",
    "        \n",
    "#         top_k_list = []\n",
    "#         top_k_score = []\n",
    "        \n",
    "#         for hit in hits:\n",
    "\n",
    "#             wiki_id = hit['_id']\n",
    "#             ctx = hit['_source']['content']\n",
    "#             score = hit['_score']\n",
    "            \n",
    "#             top_k_list.append(wiki_context_id_pair[wiki_id_context_pair[wiki_id]])\n",
    "#             top_k_score.append(score)\n",
    "        \n",
    "#         valid_data_with_elastic[valid_query[i]] = top_k_list\n",
    "#         valid_data_with_elastic_score[valid_query[i]] = top_k_score\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         error_questions.append([e, valid_query[i]])\n",
    "        \n",
    "#         top_n_id_text = []\n",
    "#         top_n_text = bm25.get_top_n(tokenizer.tokenize(valid_query[i]), wiki_corpus, n=top_k_num)\n",
    "        \n",
    "#         for ctx in top_n_text:\n",
    "#             #wiki_id = wiki_dataset['document_id'][wiki_dataset['text'] == ctx]\n",
    "#             top_n_id_text.append(wiki_context_id_pair[ctx])\n",
    "#         #train_data_with_elastic[valid_query[i]] = bm25.get_top_n(tokenizer.tokenize(valid_query[i]), wiki_corpus, n=top_k_num)\n",
    "        \n",
    "#         valid_data_with_elastic[valid_query[i]] = top_n_id_text\n",
    "#         temp_score = bm25.get_scores(tokenizer.tokenize(valid_query[i])).tolist()\n",
    "#         temp_score.sort(reverse=True)\n",
    "#         valid_data_with_elastic_score[valid_query[i]] = temp_score[:top_k_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/opt/ml/data/elastic_train_1000.bin', \"wb\") as file:\n",
    "#     pickle.dump(train_data_with_elastic, file)\n",
    "# with open('/opt/ml/data/elastic_train_score_1000.bin', \"wb\") as file:\n",
    "#     pickle.dump(train_data_with_elastic_score, file)\n",
    "with open('/opt/ml/data/elastic_valid_300.bin', \"wb\") as file:\n",
    "    pickle.dump(valid_data_with_elastic, file)\n",
    "with open('/opt/ml/data/elastic_valid_score_300.bin', \"wb\") as file:\n",
    "    pickle.dump(valid_data_with_elastic_score, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = load_from_disk('/opt/ml/data/test_dataset/validation').to_pandas()\n",
    "test_query = test_dataset['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [01:06<00:00,  8.97it/s]\n"
     ]
    }
   ],
   "source": [
    "test_data_with_elastic = {}\n",
    "test_data_with_elastic_score = {}\n",
    "error_test_questions = []\n",
    "top_k_num = 300\n",
    "\n",
    "for i in tqdm(range(len(test_query))):\n",
    "    try:\n",
    "        query = {\n",
    "            \"query\": {\n",
    "                \"bool\": {\n",
    "                    \"must\": [{\"match\": {\"text\": valid_query[i]}}],\n",
    "                    \"should\": [\n",
    "                        {\n",
    "                            \"match\": {\n",
    "                                \"text\": \" \".join(\n",
    "                                    [i[0] for i in ner(valid_query[i]) if i[1] != \"O\"]\n",
    "                                )\n",
    "                            }\n",
    "                        }\n",
    "                    ],\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        res = es.search(index=INDEX_NAME, q=test_query[i], size=top_k_num)\n",
    "        hits = res['hits']['hits']\n",
    "        \n",
    "        top_k_list = []\n",
    "        top_k_score = []\n",
    "        \n",
    "        for hit in hits:\n",
    "\n",
    "            wiki_id = hit['_id']\n",
    "            ctx = hit['_source']['content']\n",
    "            score = hit['_score']\n",
    "            \n",
    "            top_k_list.append(wiki_context_id_pair[ctx])\n",
    "            top_k_score.append(score)\n",
    "        \n",
    "        test_data_with_elastic[test_query[i]] = top_k_list\n",
    "        test_data_with_elastic_score[test_query[i]] = top_k_score\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_questions.append([e, test_query[i]])\n",
    "        \n",
    "        top_n_id_text = []\n",
    "        top_n_text = bm25.get_top_n(tokenizer.tokenize(test_query[i]), wiki_corpus, n=top_k_num)\n",
    "        \n",
    "        for ctx in top_n_text:\n",
    "            #wiki_id = wiki_dataset['document_id'][wiki_dataset['text'] == ctx]\n",
    "            top_n_id_text.append(wiki_context_id_pair[ctx])\n",
    "        #train_data_with_elastic[test_query[i]] = bm25.get_top_n(tokenizer.tokenize(test_query[i]), wiki_corpus, n=top_k_num)\n",
    "        \n",
    "        test_data_with_elastic[test_query[i]] = top_n_id_text\n",
    "        temp_score = bm25.get_scores(tokenizer.tokenize(test_query[i])).tolist()\n",
    "        temp_score.sort(reverse=True)\n",
    "        test_data_with_elastic_score[test_query[i]] = temp_score[:top_k_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/opt/ml/data/elastic_test_300.bin', \"wb\") as file:\n",
    "    pickle.dump(test_data_with_elastic, file)\n",
    "with open('/opt/ml/data/elastic_test_score_300.bin', \"wb\") as file:\n",
    "    pickle.dump(test_data_with_elastic_score, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/opt/ml/data/elastic_train_100.bin', \"rb\") as file:\n",
    "#     train_data_with_elastic = pickle.load(file)\n",
    "with open('/opt/ml/data/elastic_valid_200.bin', \"rb\") as file:\n",
    "    valid_data_with_elastic = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_NAME = \"wiki_index\"\n",
    "INDEX_SETTINGS = {\n",
    "  \"settings\" : {\n",
    "    \"index\":{\n",
    "      \"analysis\":{\n",
    "        \"analyzer\":{\n",
    "          \"korean\":{\n",
    "            \"type\":\"custom\",\n",
    "            \"tokenizer\":\"nori_tokenizer\",\n",
    "            \"filter\": [ \"nori_posfilter\",\"nori_readingform\",\"shingle\"],\n",
    "          }\n",
    "        },\n",
    "        \"filter\":{\n",
    "          \"nori_posfilter\":{\n",
    "            \"type\":\"nori_part_of_speech\",\n",
    "            \"stoptags\":[\n",
    "                #\"E\", # 어미\n",
    "                #\"IC\", # 감탄사\n",
    "                #\"J\", # 조사\n",
    "                #\"MAJ\" # 접속부사\n",
    "                #\"MAG\", # 일반부사\n",
    "                #\"MM\", #관형사\n",
    "                \"NA\", #알려지지 않은\n",
    "                #\"NR\", #수사\n",
    "                #\"SC\",#구분기호\n",
    "                #\"SE\",#생략\n",
    "                #\"SF\",#마침표, 물음표, 느낌표\n",
    "                \"SH\",#한자\n",
    "                \"SL\",#외국어\n",
    "                #\"SN\",#숫자\n",
    "                \"SP\",#띄어쓰기\n",
    "                \"SSC\",#닫는괄호\n",
    "                \"SSO\",#여는괄호\n",
    "                \"SY\",#기타기호\n",
    "                \"UNA\",#알수 없는\n",
    "                \"UNKNOWN\",#알수없는\n",
    "                #\"VA\",#형용사\n",
    "                #\"VCN\",#부정지정사\n",
    "                #\"VCP\",#긍정 지정사\n",
    "                \"VSV\",#알수없는\n",
    "                #\"VV\",#동사\n",
    "                #\"VX\",#보조동사 또는 형용사\n",
    "                #\"XPN\",#접두사\n",
    "                #\"XR\",#어근\n",
    "                #\"XSA\",#형용사 접미사\n",
    "                #\"XSN\",#명사 접미사\n",
    "                #\"XSV\"#동사 접미사\n",
    "            ]\n",
    "          }\n",
    "        }\n",
    "        }\n",
    "      },\n",
    "  },\n",
    "  \"mappings\": {\n",
    "      \"properties\" : {\n",
    "        \"content\" : {\n",
    "          \"type\" : \"text\",\n",
    "          \"analyzer\": \"korean\",\n",
    "          \"search_analyzer\": \"korean\"\n",
    "        },\n",
    "        \"title\" : {\n",
    "          \"type\" : \"text\",\n",
    "          \"analyzer\": \"korean\",\n",
    "          \"search_analyzer\": \"korean\"\n",
    "        }\n",
    "      }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gq negative sampling\n",
    "\n",
    "# retrieval_data_with_elastic = {}\n",
    "# retrieval_data_with_elastic_score = {}\n",
    "# error_questions = []\n",
    "# top_k_num = 101\n",
    "# for i in tqdm(range(len(retrieval_query))):\n",
    "#     try:\n",
    "\n",
    "#         res = es.search(index=INDEX_NAME, q=retrieval_query[i], size=top_k_num)\n",
    "#         hits = res['hits']['hits']\n",
    "\n",
    "#         top_k_list = []\n",
    "#         top_k_score = []\n",
    "\n",
    "#         for hit in hits:\n",
    "\n",
    "#             wiki_id = hit['_id']\n",
    "#             #ctx = hit['_source']['content']\n",
    "#             score = hit['_score']\n",
    "\n",
    "#             top_k_list.append(int(wiki_id))\n",
    "#             top_k_score.append(score)\n",
    "\n",
    "#         if len(top_k_list) == top_k_num:\n",
    "#             retrieval_data_with_elastic[retrieval_query[i]] = top_k_list\n",
    "#             retrieval_data_with_elastic_score[retrieval_query[i]] = top_k_score\n",
    "#         else:\n",
    "#             print('under_top_k')\n",
    "#             print(retrieval_query[i])\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print('elastic_error')\n",
    "#         print(retrieval_query[i])\n",
    "\n",
    "#         error_questions.append([e, retrieval_query[i]])\n",
    "\n",
    "#         top_n_id_text = []\n",
    "#         top_n_text = bm25.get_top_n(tokenizer.tokenize(retrieval_query[i]), wiki_corpus, n=top_k_num)\n",
    "#         if len(top_n_text) == top_k_num:\n",
    "#             for ctx in top_n_text:\n",
    "\n",
    "#                 wiki_id = wiki_dataset['document_id'][wiki_dataset['text'] == ctx]\n",
    "#                 top_n_id_text.append(int(wiki_id))\n",
    "\n",
    "#             #train_data_with_elastic[retrieval_query[i]] = bm25.get_top_n(tokenizer.tokenize(retrieval_query[i]), wiki_corpus, n=top_k_num)\n",
    "#             retrieval_data_with_elastic[retrieval_query[i]] = top_n_id_text\n",
    "#             temp_score = bm25.get_scores(tokenizer.tokenize(retrieval_query[i])).tolist()\n",
    "#             temp_score.sort(reverse=True)\n",
    "#             retrieval_data_with_elastic_score[retrieval_query[i]] = temp_score[:top_k_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
