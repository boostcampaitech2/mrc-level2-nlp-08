{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!service elasticsearch start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, Dataset\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "#from retrieval_model import BertEncoder\n",
    "from transformers import AdamW, TrainingArguments, get_linear_schedule_with_warmup\n",
    "from torch import nn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from torch.utils.data import (DataLoader, RandomSampler, TensorDataset, SequentialSampler)\n",
    "from tqdm import tqdm, trange\n",
    "import pickle\n",
    "import os, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_from_disk(\"/opt/ml/data/new_train_dataset/train\")\n",
    "valid_dataset = load_from_disk(\"/opt/ml/data/train_dataset/validation\")\n",
    "test_dataset = load_from_disk('/opt/ml/data/test_dataset/validation')\n",
    "\n",
    "wiki_dataset = pd.read_json('data/wikipedia_documents.json',orient='index') # wiki context\n",
    "data_path = \"data/\"\n",
    "context_path = \"wikipedia_documents.json\"\n",
    "with open(os.path.join(data_path, context_path), \"r\", encoding=\"utf-8\") as f:\n",
    "    wiki = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/wikipedia_documents.json', \"r\", encoding=\"utf-8\") as f:\n",
    "    wiki = json.load(f)\n",
    "contexts = list(dict.fromkeys([v[\"text\"] for v in wiki.values()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_corpus = list(set(valid_dataset['context']))\n",
    "train_corpus = list(set(train_dataset['context']))\n",
    "wiki_corpus = list(set(wiki_dataset))\n",
    "query = train_dataset['question']\n",
    "test_query = test_dataset['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "import pprint  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "INDEX_NAME = \"wiki_index\"\n",
    "\n",
    "INDEX_SETTINGS = {\n",
    "  \"settings\" : {\n",
    "    \"index\":{\n",
    "      \"analysis\":{\n",
    "        \"analyzer\":{\n",
    "          \"korean\":{\n",
    "            \"type\":\"custom\",\n",
    "            \"tokenizer\":\"nori_tokenizer\",\n",
    "            \"filter\": [ \"shingle\" ],\n",
    "\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"mappings\": {\n",
    "\n",
    "      \"properties\" : {\n",
    "        \"content\" : {\n",
    "          \"type\" : \"text\",\n",
    "          \"analyzer\": \"korean\",\n",
    "          \"search_analyzer\": \"korean\"\n",
    "        },\n",
    "        \"title\" : {\n",
    "          \"type\" : \"text\",\n",
    "          \"analyzer\": \"korean\",\n",
    "          \"search_analyzer\": \"korean\"\n",
    "        }\n",
    "      }\n",
    "\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    {\n",
    "        '_index' : INDEX_NAME,\n",
    "        '_id' : i,\n",
    "        'content' : wiki_dataset.iloc[i]['text']\n",
    "    }\n",
    "    for i in range(wiki_dataset.shape[0])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    {\n",
    "        '_index' : INDEX_NAME,\n",
    "        '_id' : i,\n",
    "        'content' : contexts[i]\n",
    "    }\n",
    "    for i in range(len(contexts))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    es.transport.close()\n",
    "except:\n",
    "    pass\n",
    "es = Elasticsearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/bc/lib/python3.8/site-packages/elasticsearch/connection/base.py:209: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.15/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': '3ef0ae13ffd8',\n",
       " 'cluster_name': 'elasticsearch',\n",
       " 'cluster_uuid': 'QBJxOfIgS5mBwdLwFxXqyg',\n",
       " 'version': {'number': '7.15.1',\n",
       "  'build_flavor': 'default',\n",
       "  'build_type': 'deb',\n",
       "  'build_hash': '83c34f456ae29d60e94d886e455e6a3409bba9ed',\n",
       "  'build_date': '2021-10-07T21:56:19.031608185Z',\n",
       "  'build_snapshot': False,\n",
       "  'lucene_version': '8.9.0',\n",
       "  'minimum_wire_compatibility_version': '6.8.0',\n",
       "  'minimum_index_compatibility_version': '6.0.0-beta1'},\n",
       " 'tagline': 'You Know, for Search'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10202/2824541921.py:1: DeprecationWarning: Using positional arguments for APIs is deprecated and will be disabled in 8.0.0. Instead use only keyword arguments for all APIs. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  if es.indices.exists(INDEX_NAME):\n",
      "/tmp/ipykernel_10202/2824541921.py:3: DeprecationWarning: The 'body' parameter is deprecated for the 'create' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  es.indices.create(index=INDEX_NAME, body=INDEX_SETTINGS)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'wiki_index'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if es.indices.exists(INDEX_NAME):\n",
    "    es.indices.delete(index=INDEX_NAME)\n",
    "es.indices.create(index=INDEX_NAME, body=INDEX_SETTINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESPONSE: (60613, [])\n",
      "154.62924194335938\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "try:\n",
    "    response = helpers.bulk(es, docs)\n",
    "    print (\"\\nRESPONSE:\", response)\n",
    "except Exception as e:\n",
    "    print(\"\\nERROR:\", e)\n",
    "    \n",
    "end_time = time.time()\n",
    "\n",
    "print(end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions[1197] = 'SW 취약점 찾기 대회에서 수상한 해커그룹들이 찾은 보안 취약점의 특징은?'\n",
    "questions[3] = '11~12 세기에 제작된 본존불은 보통 어떤 나라의 특징이 전파되었나요?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3952it [03:49, 17.20it/s]\n"
     ]
    }
   ],
   "source": [
    "error_questions = []\n",
    "train_neg = [[] for _ in range(len(questions))]\n",
    "\n",
    "for idx, (question, answer) in tqdm(enumerate(zip(questions, answers))):\n",
    "    try:\n",
    "        res = es.search(index=INDEX_NAME, q=question, size=30)\n",
    "    except Exception as e:\n",
    "        error_questions.append([e, question])\n",
    "    for hits in res['hits']['hits']:\n",
    "        train_neg[idx].append(hits['_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx1 in range(len(train_neg)):\n",
    "    for idx2 in range(train_neg[0]):\n",
    "        train_neg[idx1][idx2] = int(train_neg[idx1][idx2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"elastic_train_neg\", \"wb\") as file:\n",
    "    pickle.dump(train_neg, file)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8cf67b00fe44a7bbb95b84fa7b31b362c005e968b891258bc8597366f52a1c53"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('bc': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
