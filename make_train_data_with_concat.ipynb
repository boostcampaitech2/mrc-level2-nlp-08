{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Retrieval.retrieval import SparseRetrieval\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_from_disk, Dataset\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "from clean_dataset import preprocess\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (969 > 512). Running this sequence through the model will result in indexing errors\n",
      "/opt/conda/lib/python3.8/site-packages/elasticsearch/connection/base.py:209: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.15/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"kykim/bert-kor-base\") # 토크나이저\n",
    "sparse_retrieval = SparseRetrieval(tokenizer=tokenizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=\"/opt/ml/mrc-level2-nlp-08/Retrieval/\"\n",
    "caching_path = \"caching/\"\n",
    "wiki_dataset = pd.read_json(\"/opt/ml/data/preprocess_wiki.json\", orient=\"index\") # 전처리된 위키 데이터\n",
    "train_dataset = load_from_disk(\"/opt/ml/data/new_train_dataset/train\").to_pandas() # 전처리된 train data\n",
    "origin_train_dataset = load_from_disk(\"/opt/ml/data/train_dataset/train\").to_pandas() # 전처리 되지 않은 train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "caching_context_id_path = data_path + caching_path + \"wiki_context_id_pair.bin\"\n",
    "caching_id_context_path = data_path + caching_path + \"wiki_id_context_pair.bin\"\n",
    "if (os.path.isfile(caching_context_id_path) and os.path.isfile(caching_id_context_path)):\n",
    "    with open(caching_context_id_path, \"rb\") as f:\n",
    "        wiki_context_id_dict = pickle.load(f)\n",
    "    with open(caching_id_context_path, \"rb\") as f:\n",
    "        wiki_id_context_dict = pickle.load(f)\n",
    "else:\n",
    "    wiki_text = wiki_dataset[\"text\"]\n",
    "    wiki_id = wiki_dataset[\"document_id\"]\n",
    "    wiki_context_id_dict = {k: v for k, v in zip(wiki_text, wiki_id)}\n",
    "    wiki_id_context_dict = {k: v for k, v in zip(wiki_id, wiki_text)}\n",
    "    with open(caching_context_id_path, \"wb\") as file:\n",
    "        pickle.dump(wiki_context_id_dict, file)\n",
    "    with open(caching_id_context_path, \"wb\") as file:\n",
    "        pickle.dump(wiki_id_context_dict, file)\n",
    "\n",
    "\n",
    "# caching 된 dict가 없을 경우 만들어서 caching을 진행합니다.\n",
    "# wiki_context_id : key : context, value: wiki id\n",
    "# wiki_id_context : key : wiki id, value: context\n",
    "# -> retrieval로 가지고온 id를 context로 변환하는 역할을 합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3952 [00:00<?, ?it/s]/opt/conda/lib/python3.8/site-packages/elasticsearch/connection/base.py:209: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.15/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "100%|██████████| 3952/3952 [04:53<00:00, 13.48it/s]\n"
     ]
    }
   ],
   "source": [
    "retrieval_ids,retrieval_scores = sparse_retrieval.get_topk_doc_id_and_score_for_querys(train_dataset['question'].to_list(),top_k=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3952/3952 [00:00<00:00, 22261.29it/s]\n"
     ]
    }
   ],
   "source": [
    "new_context = []\n",
    "\n",
    "for i in tqdm(range(len(train_dataset))):\n",
    "\n",
    "    train_context = train_dataset['context'][i] # ground truth\n",
    "    \n",
    "    query = train_dataset['question'][i] # query\n",
    "    ctx_wiki_ids = retrieval_ids[query] # sparse_retrieval[query] = doc_ids\n",
    "    answer = train_dataset['answers'][i]['text'][0]\n",
    "    \n",
    "    cnt = 4 # 추가할 갯수\n",
    "    train_concat_list = [origin_train_dataset['context'][i]]\n",
    "\n",
    "    pre_ground = preprocess(train_context)\n",
    "\n",
    "    idx = 0\n",
    "\n",
    "    while cnt != 0:\n",
    "        concat_cxt = wiki_id_context_dict[ctx_wiki_ids[idx]] # id 를 cxt로 변환\n",
    "\n",
    "        if pre_ground != concat_cxt and not (answer in concat_cxt):\n",
    "            # 비슷한 context를 추가하되 정답을 포함하지 않는 문장을 추가한다.\n",
    "            train_concat_list.append(concat_cxt)\n",
    "            cnt -= 1\n",
    "        idx += 1\n",
    "        if idx == 200: # index를 넘어가면 break\n",
    "            break\n",
    "    add_sim_context = ' '.join(train_concat_list)\n",
    "    new_context.append(add_sim_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/opt/ml/data/new_train_dataset/'\n",
    "save_name = 'train_concat_dataset'\n",
    "train_df = load_from_disk(save_path + \"train\").to_pandas()\n",
    "train_df['context'] = new_context\n",
    "concat_train_dataset = Dataset.from_pandas(train_df)\n",
    "concat_train_dataset.save_to_disk(save_path + save_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(save_path + 'dataset_dict.json') as f:\n",
    "    dataset_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'splits': ['train', 'validation', 'train_concat_no_duplication', 'SEP_train', 'train_pre_es_no_dup_wiki', 'train_ori_es_no_dup_wiki', 'train_concat_es_no_dup_wiki', 'train_concat_es_no_dup_more_wiki']}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'splits': ['train', 'validation', 'train_concat_no_duplication', 'SEP_train', 'train_pre_es_no_dup_wiki', 'train_ori_es_no_dup_wiki', 'train_concat_es_no_dup_wiki', 'train_concat_es_no_dup_more_wiki', 'train_concat_dataset']}\n"
     ]
    }
   ],
   "source": [
    "dataset_dict['splits'].append(save_name)\n",
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_path + 'dataset_dict.json', \"w\", encoding=\"utf-8\") as make_file:\n",
    "    json.dump(dataset_dict, make_file, indent=\"\\t\", ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
