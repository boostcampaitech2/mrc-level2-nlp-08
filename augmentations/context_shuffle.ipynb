{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('elastic_train_1000.bin','rb') as f:\n",
    "    es_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/opt/ml/develop/hybrid_retrieval_500 (1).bin', 'rb') as f:\n",
    "    hb_v7 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../data/preprocess_wiki.json', \"r\", encoding=\"utf-8\") as f:\n",
    "    wiki = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(es_train.values())[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test dataset shuffle concatenated passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "hb_q = list(hb_v7.keys())\n",
    "hb_p = list(hb_v7.values())\n",
    "test_d = load_from_disk('../data/test_dataset/validation')\n",
    "\n",
    "test_concat = []\n",
    "for i, q in enumerate(hb_q):\n",
    "    concat_ids = []\n",
    "    for j, p_id in enumerate(hb_p[i]):\n",
    "        concat_ids.append(p_id)\n",
    "        if len(concat_ids) == 20:\n",
    "            break\n",
    "    \n",
    "    test_concat.append(concat_ids)\n",
    "\n",
    "len(test_concat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests_0 = test_concat\n",
    "tests_1 = []\n",
    "tests_2 = []\n",
    "tests_3 = []\n",
    "tests_4 = []\n",
    "tests_5 = []\n",
    "tests_6 = []\n",
    "tests_7 = []\n",
    "tests_8 = []\n",
    "tests_9 = []\n",
    "\n",
    "for i in range(len(test_concat)):\n",
    "    test_1 = test_concat[i].copy()\n",
    "    test_1[0], test_1[1] = test_1[1], test_1[0]\n",
    "\n",
    "    test_2 = test_concat[i].copy()\n",
    "    test_2[0], test_2[2] = test_2[2], test_2[0]\n",
    "\n",
    "    test_3 = test_concat[i].copy()\n",
    "    test_3[0], test_3[3] = test_3[3], test_3[0]\n",
    "\n",
    "    test_4 = test_concat[i].copy()\n",
    "    test_4[0], test_4[4] = test_4[4], test_4[0]\n",
    "\n",
    "    test_5 = test_concat[i].copy()\n",
    "    test_5[0], test_5[5] = test_5[5], test_5[0]\n",
    "\n",
    "    test_6 = test_concat[i].copy()\n",
    "    test_6[0], test_6[6] = test_6[6], test_6[0]\n",
    "\n",
    "    test_7 = test_concat[i].copy()\n",
    "    test_7[0], test_7[7] = test_7[7], test_7[0]\n",
    "\n",
    "    test_8 = test_concat[i].copy()\n",
    "    test_8[0], test_8[8] = test_8[8], test_8[0]\n",
    "\n",
    "    test_9 = test_concat[i].copy()\n",
    "    test_9[0], test_9[9] = test_9[9], test_9[0]\n",
    "\n",
    "    tests_1.append(test_1)\n",
    "    tests_2.append(test_2)\n",
    "    tests_3.append(test_3)\n",
    "    tests_4.append(test_4)\n",
    "    tests_5.append(test_5)\n",
    "    tests_6.append(test_6)\n",
    "    tests_7.append(test_7)\n",
    "    tests_8.append(test_8)\n",
    "    tests_9.append(test_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tests_0[0])\n",
    "print(tests_1[0])\n",
    "print(tests_2[0])\n",
    "print(tests_3[0])\n",
    "print(tests_4[0])\n",
    "print(tests_5[0])\n",
    "print(tests_6[0])\n",
    "print(tests_7[0])\n",
    "print(tests_8[0])\n",
    "print(tests_9[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/wikipedia_documents.json', \"r\", encoding=\"utf-8\") as f:\n",
    "    origin_wiki = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_df = pd.DataFrame(test_d)\n",
    "\n",
    "tests_1_df = test_df.copy()\n",
    "tests_2_df = test_df.copy()\n",
    "tests_3_df = test_df.copy()\n",
    "tests_4_df = test_df.copy()\n",
    "tests_5_df = test_df.copy()\n",
    "tests_6_df = test_df.copy()\n",
    "tests_7_df = test_df.copy()\n",
    "tests_8_df = test_df.copy()\n",
    "tests_9_df = test_df.copy()\n",
    "\n",
    "contexts = []\n",
    "for i, row in tests_1_df.iterrows():\n",
    "    context = ''\n",
    "    for j in range(len(tests_1[i])):\n",
    "        context += ' ' + wiki[str(tests_1[i][j])]['text']\n",
    "    contexts.append(context)\n",
    "tests_1_df['context'] = contexts\n",
    "\n",
    "contexts = []\n",
    "for i, row in tests_2_df.iterrows():\n",
    "    context = ''\n",
    "    for j in range(len(tests_2[i])):\n",
    "        context += ' ' + wiki[str(tests_2[i][j])]['text']\n",
    "    contexts.append(context)\n",
    "tests_2_df['context'] = contexts\n",
    "\n",
    "contexts = []\n",
    "for i, row in tests_3_df.iterrows():\n",
    "    context = ''\n",
    "    for j in range(len(tests_3[i])):\n",
    "        context += ' ' + wiki[str(tests_3[i][j])]['text']\n",
    "    contexts.append(context)\n",
    "tests_3_df['context'] = contexts\n",
    "\n",
    "contexts = []\n",
    "for i, row in tests_4_df.iterrows():\n",
    "    context = ''\n",
    "    for j in range(len(tests_4[i])):\n",
    "        context += ' ' + wiki[str(tests_4[i][j])]['text']\n",
    "    contexts.append(context)\n",
    "tests_4_df['context'] = contexts\n",
    "\n",
    "contexts = []\n",
    "for i, row in tests_5_df.iterrows():\n",
    "    context = ''\n",
    "    for j in range(len(tests_5[i])):\n",
    "        context += ' ' + wiki[str(tests_5[i][j])]['text']\n",
    "    contexts.append(context)\n",
    "tests_5_df['context'] = contexts\n",
    "\n",
    "contexts = []\n",
    "for i, row in tests_6_df.iterrows():\n",
    "    context = ''\n",
    "    for j in range(len(tests_6[i])):\n",
    "        context += ' ' + wiki[str(tests_6[i][j])]['text']\n",
    "    contexts.append(context)\n",
    "tests_6_df['context'] = contexts\n",
    "\n",
    "contexts = []\n",
    "for i, row in tests_7_df.iterrows():\n",
    "    context = ''\n",
    "    for j in range(len(tests_7[i])):\n",
    "        context += ' ' + wiki[str(tests_7[i][j])]['text']\n",
    "    contexts.append(context)\n",
    "tests_7_df['context'] = contexts\n",
    "\n",
    "contexts = []\n",
    "for i, row in tests_8_df.iterrows():\n",
    "    context = ''\n",
    "    for j in range(len(tests_8[i])):\n",
    "        context += ' ' + wiki[str(tests_8[i][j])]['text']\n",
    "    contexts.append(context)\n",
    "tests_8_df['context'] = contexts\n",
    "\n",
    "contexts = []\n",
    "for i, row in tests_9_df.iterrows():\n",
    "    context = ''\n",
    "    for j in range(len(tests_9[i])):\n",
    "        context += ' ' + wiki[str(tests_9[i][j])]['text']\n",
    "    contexts.append(context)\n",
    "tests_9_df['context'] = contexts\n",
    "\n",
    "tests_1_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "Dataset.from_pandas(tests_1_df).save_to_disk(\"../test_shuffle_v2/test_1\")\n",
    "Dataset.from_pandas(tests_2_df).save_to_disk(\"../test_shuffle_v2/test_2\")\n",
    "Dataset.from_pandas(tests_3_df).save_to_disk(\"../test_shuffle_v2/test_3\")\n",
    "Dataset.from_pandas(tests_4_df).save_to_disk(\"../test_shuffle_v2/test_4\")\n",
    "Dataset.from_pandas(tests_5_df).save_to_disk(\"../test_shuffle_v2/test_5\")\n",
    "Dataset.from_pandas(tests_6_df).save_to_disk(\"../test_shuffle_v2/test_6\")\n",
    "Dataset.from_pandas(tests_7_df).save_to_disk(\"../test_shuffle_v2/test_7\")\n",
    "Dataset.from_pandas(tests_8_df).save_to_disk(\"../test_shuffle_v2/test_8\")\n",
    "Dataset.from_pandas(tests_9_df).save_to_disk(\"../test_shuffle_v2/test_9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train dataset shuffles concatenated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "train = load_from_disk('../data/train_dataset')['train']\n",
    "train[0]['document_id']\n",
    "train_ground_truth_doc_ids = train['document_id']\n",
    "len(train_ground_truth_doc_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_question = list(es_train.keys())\n",
    "es_passage = list(es_train.values())\n",
    "\n",
    "shuffles = []\n",
    "for i, q in enumerate(es_question):\n",
    "    gt_id = train_ground_truth_doc_ids[i]\n",
    "    answer = train[i]['answers']['text'][0]\n",
    "\n",
    "    concat_ids = [gt_id]\n",
    "    for j, p_id in enumerate(es_passage[i]):\n",
    "        if gt_id != p_id and p_id not in concat_ids and answer not in wiki[str(p_id)]:\n",
    "                concat_ids.append(p_id)\n",
    "        if len(concat_ids) == 5:\n",
    "            break\n",
    "    \n",
    "    shuffles.append(concat_ids)\n",
    "\n",
    "len(shuffles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffles[1029]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffles_gt_0 = shuffles\n",
    "shuffles_gt_1 = []\n",
    "shuffles_gt_2 = []\n",
    "shuffles_gt_3 = []\n",
    "shuffles_gt_4 = []\n",
    "\n",
    "for i in range(len(shuffles)):\n",
    "    shuffle_gt_1 = shuffles[i].copy()\n",
    "    shuffle_gt_1[0], shuffle_gt_1[1] = shuffle_gt_1[1], shuffle_gt_1[0]\n",
    "\n",
    "    shuffle_gt_2 = shuffles[i].copy()\n",
    "    tmp_gt = shuffle_gt_2.pop(0)\n",
    "    shuffle_gt_2.insert(2, tmp_gt)\n",
    "\n",
    "    shuffle_gt_3 = shuffles[i].copy()\n",
    "    tmp_gt = shuffle_gt_3.pop(0)\n",
    "    shuffle_gt_3.insert(3, tmp_gt)\n",
    "\n",
    "    shuffle_gt_4 = shuffles[i].copy()\n",
    "    tmp_gt = shuffle_gt_4.pop(0)\n",
    "    shuffle_gt_4.append(tmp_gt)\n",
    "\n",
    "    shuffles_gt_1.append(shuffle_gt_1)\n",
    "    shuffles_gt_2.append(shuffle_gt_2)\n",
    "    shuffles_gt_3.append(shuffle_gt_3)\n",
    "    shuffles_gt_4.append(shuffle_gt_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shuffles_gt_0[0])\n",
    "print(shuffles_gt_1[0])\n",
    "print(shuffles_gt_2[0])\n",
    "print(shuffles_gt_3[0])\n",
    "print(shuffles_gt_4[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(shuffles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wiki[str(shuffles_gt_1[0][:1][0])]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffles_gt_1_answers = {}\n",
    "shuffles_gt_2_answers = {}\n",
    "shuffles_gt_3_answers = {}\n",
    "shuffles_gt_4_answers = {}\n",
    "\n",
    "for i in range(len(shuffles)):\n",
    "    answers = train[i]['answers']\n",
    "    passage_len = 0\n",
    "    for concat_wiki_id in shuffles_gt_1[i][:1]:\n",
    "        passage_len += len(wiki[str(concat_wiki_id)]['text'])\n",
    "    answers['answer_start'] = [passage_len + 1 + answers['answer_start'][0]] # insert one space when concatenate\n",
    "    shuffles_gt_1_answers[i] = answers\n",
    "\n",
    "    answers = train[i]['answers']\n",
    "    passage_len = 0\n",
    "    for concat_wiki_id in shuffles_gt_2[i][:2]:\n",
    "        passage_len += len(wiki[str(concat_wiki_id)]['text'])\n",
    "    answers['answer_start'] = [passage_len + 2 + answers['answer_start'][0]] # insert two space when concatenate\n",
    "    shuffles_gt_2_answers[i] = answers\n",
    "\n",
    "    answers = train[i]['answers']\n",
    "    passage_len = 0\n",
    "    for concat_wiki_id in shuffles_gt_3[i][:3]:\n",
    "        passage_len += len(wiki[str(concat_wiki_id)]['text'])\n",
    "    answers['answer_start'] = [passage_len + 3 + answers['answer_start'][0]] # insert three space when concatenate\n",
    "    shuffles_gt_3_answers[i] = answers\n",
    "\n",
    "    answers = train[i]['answers']\n",
    "    passage_len = 0\n",
    "    for concat_wiki_id in shuffles_gt_4[i][:4]:\n",
    "        passage_len += len(wiki[str(concat_wiki_id)]['text'])\n",
    "    answers['answer_start'] = [passage_len + 4 + answers['answer_start'][0]] # insert four space when concatenate\n",
    "    shuffles_gt_4_answers[i] = answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shuffles_gt_1_answers[0])\n",
    "print(shuffles_gt_2_answers[0])\n",
    "print(shuffles_gt_3_answers[0])\n",
    "print(shuffles_gt_4_answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# new_train = load_from_disk('../data/new_train_dataset')['train']\n",
    "\n",
    "train_df = pd.DataFrame(train)\n",
    "\n",
    "concat_train_df_gt_0 = train_df.copy()\n",
    "concat_train_df_gt_1 = train_df.copy()\n",
    "concat_train_df_gt_2 = train_df.copy()\n",
    "concat_train_df_gt_3 = train_df.copy()\n",
    "concat_train_df_gt_4 = train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/wikipedia_documents.json', \"r\", encoding=\"utf-8\") as f:\n",
    "    origin_wiki = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = []\n",
    "for i, row in concat_train_df_gt_0.iterrows():\n",
    "    context = origin_wiki[str(shuffles_gt_0[i][0])]['text']\n",
    "    for j in range(1, len(shuffles_gt_0[i])):\n",
    "        context += ' ' + wiki[str(shuffles_gt_0[i][j])]['text']\n",
    "    contexts.append(context)\n",
    "concat_train_df_gt_0['context'] = contexts\n",
    "\n",
    "contexts = []\n",
    "for i, row in concat_train_df_gt_1.iterrows():\n",
    "    context = wiki[str(shuffles_gt_1[i][0])]['text']\n",
    "    context += ' ' + origin_wiki[str(shuffles_gt_1[i][1])]['text']\n",
    "    for j in range(2, len(shuffles_gt_1[i])):\n",
    "        context += ' ' + wiki[str(shuffles_gt_1[i][j])]['text']\n",
    "    contexts.append(context)\n",
    "concat_train_df_gt_1['context'] = contexts\n",
    "concat_train_df_gt_1['answers'] = shuffles_gt_1_answers.values()\n",
    "\n",
    "contexts = []\n",
    "for i, row in concat_train_df_gt_2.iterrows():\n",
    "    context = wiki[str(shuffles_gt_2[i][0])]['text']\n",
    "    context += ' ' + wiki[str(shuffles_gt_2[i][1])]['text']\n",
    "    context += ' ' + origin_wiki[str(shuffles_gt_2[i][2])]['text']\n",
    "    for j in range(3, len(shuffles_gt_2[i])):\n",
    "        context += ' ' + wiki[str(shuffles_gt_2[i][j])]['text']\n",
    "    contexts.append(context)\n",
    "concat_train_df_gt_2['context'] = contexts\n",
    "concat_train_df_gt_2['answers'] = shuffles_gt_2_answers.values()\n",
    "\n",
    "contexts = []\n",
    "for i, row in concat_train_df_gt_3.iterrows():\n",
    "    context = wiki[str(shuffles_gt_3[i][0])]['text']\n",
    "    context += ' ' + wiki[str(shuffles_gt_3[i][1])]['text']\n",
    "    context += ' ' + wiki[str(shuffles_gt_3[i][2])]['text']\n",
    "    context += ' ' + origin_wiki[str(shuffles_gt_3[i][3])]['text']\n",
    "    for j in range(4, len(shuffles_gt_3[i])):\n",
    "        context += ' ' + wiki[str(shuffles_gt_3[i][j])]['text']\n",
    "    contexts.append(context)\n",
    "concat_train_df_gt_3['context'] = contexts\n",
    "concat_train_df_gt_3['answers'] = shuffles_gt_3_answers.values()\n",
    "\n",
    "contexts = []\n",
    "for i, row in concat_train_df_gt_4.iterrows():\n",
    "    context = wiki[str(shuffles_gt_4[i][0])]['text']\n",
    "    context += ' ' + wiki[str(shuffles_gt_4[i][1])]['text']\n",
    "    context += ' ' + wiki[str(shuffles_gt_4[i][2])]['text']\n",
    "    context += ' ' + wiki[str(shuffles_gt_4[i][3])]['text']\n",
    "    context += ' ' + origin_wiki[str(shuffles_gt_4[i][4])]['text']\n",
    "    contexts.append(context)\n",
    "concat_train_df_gt_4['context'] = contexts\n",
    "concat_train_df_gt_4['answers'] = shuffles_gt_4_answers.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in concat_train_df_gt_0.iterrows():\n",
    "    if row['context'][row['answers']['answer_start'][0]] != row['answers']['text'][0][0]:\n",
    "        print(i)\n",
    "for i, row in concat_train_df_gt_1.iterrows():\n",
    "    if row['context'][row['answers']['answer_start'][0]] != row['answers']['text'][0][0]:\n",
    "        print(i)\n",
    "for i, row in concat_train_df_gt_2.iterrows():\n",
    "    if row['context'][row['answers']['answer_start'][0]] != row['answers']['text'][0][0]:\n",
    "        print(i)\n",
    "for i, row in concat_train_df_gt_3.iterrows():\n",
    "    if row['context'][row['answers']['answer_start'][0]] != row['answers']['text'][0][0]:\n",
    "        print(i)\n",
    "for i, row in concat_train_df_gt_4.iterrows():\n",
    "    if row['context'][row['answers']['answer_start'][0]] != row['answers']['text'][0][0]:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "Dataset.from_pandas(concat_train_df_gt_0).save_to_disk(\"../passage_shuffle/concat_train_gt_0\")\n",
    "Dataset.from_pandas(concat_train_df_gt_1).save_to_disk(\"../passage_shuffle/concat_train_gt_1\")\n",
    "Dataset.from_pandas(concat_train_df_gt_2).save_to_disk(\"../passage_shuffle/concat_train_gt_2\")\n",
    "Dataset.from_pandas(concat_train_df_gt_3).save_to_disk(\"../passage_shuffle/concat_train_gt_3\")\n",
    "Dataset.from_pandas(concat_train_df_gt_4).save_to_disk(\"../passage_shuffle/concat_train_gt_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_train_df_gt_0.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffles_gt_0[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fids = train_df.copy()\n",
    "\n",
    "ctxs = []\n",
    "for i, row in fids.iterrows():\n",
    "    ctx = [\n",
    "        {\n",
    "            'title': row['title'],\n",
    "            'text': origin_wiki[str(shuffles_gt_0[i][0])]['text']\n",
    "        }\n",
    "    ]\n",
    "    for j in range(1, len(shuffles_gt_0[i])):\n",
    "        ctx.append(\n",
    "            {\n",
    "                'title': wiki[str(shuffles_gt_0[i][j])]['title'],\n",
    "                'text': wiki[str(shuffles_gt_0[i][j])]['text']\n",
    "            }\n",
    "        )\n",
    "    ctxs.append(ctx)\n",
    "\n",
    "fids['ctxs'] = ctxs\n",
    "\n",
    "answers = []\n",
    "targets = []\n",
    "for i, row in fids.iterrows():\n",
    "    targets.append(row['answers']['text'][0])\n",
    "    answers.append([targets[i]])\n",
    "fids['target'] = targets\n",
    "fids['answers'] = answers\n",
    "\n",
    "fids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fids.drop(columns=['context'])\n",
    "Dataset.from_pandas(fids).save_to_disk('../fids_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vd = load_from_disk('../data/train_dataset')['validation']\n",
    "vd\n",
    "\n",
    "fid_vd = pd.DataFrame(vd)\n",
    "\n",
    "ctxs = []\n",
    "for i, row in fid_vd.iterrows():\n",
    "    ctx = [\n",
    "        {\n",
    "            'title': row['title'],\n",
    "            'text': origin_wiki[str(row['document_id'])]['text']\n",
    "        }\n",
    "    ]\n",
    "    ctxs.append(ctx)\n",
    "\n",
    "fid_vd['ctxs'] = ctxs\n",
    "\n",
    "answers = []\n",
    "targets = []\n",
    "for i, row in fid_vd.iterrows():\n",
    "    targets.append(row['answers']['text'][0])\n",
    "    answers.append([targets[i]])\n",
    "fid_vd['target'] = targets\n",
    "fid_vd['answers'] = answers\n",
    "fid_vd.drop(columns=['context'])\n",
    "fid_vd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset.from_pandas(fid_vd).save_to_disk('../fids_dataset_valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_question = list(es_train.keys())\n",
    "es_passage = list(es_train.values())\n",
    "\n",
    "shuffles = []\n",
    "for i, q in enumerate(es_question):\n",
    "    gt_id = train_ground_truth_doc_ids[i]\n",
    "    answer = train[i]['answers']['text'][0]\n",
    "\n",
    "    concat_ids = [gt_id]\n",
    "    for j, p_id in enumerate(es_passage[i]):\n",
    "        if gt_id != p_id and p_id not in concat_ids and answer not in wiki[str(p_id)]:\n",
    "                concat_ids.append(p_id)\n",
    "        if len(concat_ids) == 5:\n",
    "            break\n",
    "    \n",
    "    shuffles.append(concat_ids)\n",
    "\n",
    "len(shuffles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(shuffles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_shuffles = [shuffle for shuffle in shuffles]\n",
    "\n",
    "gt_passage_table = {}\n",
    "gt_passage_table = {i: concat_ids[0] for i, concat_ids in enumerate(random_shuffles)}\n",
    "gt_passage_table[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "for i in range(len(random_shuffles)):\n",
    "    random.shuffle(random_shuffles[i])\n",
    "random_shuffles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_shuffles[1], gt_passage_table[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_shuffles_answers = {}\n",
    "\n",
    "for i in range(len(random_shuffles)):\n",
    "    answers = train[i]['answers']\n",
    "    passage_len = 0\n",
    "\n",
    "    if random_shuffles[i].index(gt_passage_table[i]) == 0:\n",
    "        pass\n",
    "\n",
    "    elif random_shuffles[i].index(gt_passage_table[i]) == 1:\n",
    "        for concat_wiki_id in random_shuffles[i][:1]:\n",
    "            passage_len += len(wiki[str(concat_wiki_id)]['text'])\n",
    "        answers['answer_start'] = [passage_len + 1 + answers['answer_start'][0]] # insert one space when concatenate\n",
    "\n",
    "    elif random_shuffles[i].index(gt_passage_table[i]) == 2:\n",
    "        for concat_wiki_id in random_shuffles[i][:2]:\n",
    "            passage_len += len(wiki[str(concat_wiki_id)]['text'])\n",
    "        answers['answer_start'] = [passage_len + 2 + answers['answer_start'][0]] # insert two space when concatenate\n",
    "\n",
    "    elif random_shuffles[i].index(gt_passage_table[i]) == 3:\n",
    "        for concat_wiki_id in random_shuffles[i][:3]:\n",
    "            passage_len += len(wiki[str(concat_wiki_id)]['text'])\n",
    "        answers['answer_start'] = [passage_len + 3 + answers['answer_start'][0]] # insert three space when concatenate\n",
    "\n",
    "    elif random_shuffles[i].index(gt_passage_table[i]) == 4:\n",
    "        for concat_wiki_id in random_shuffles[i][:4]:\n",
    "            passage_len += len(wiki[str(concat_wiki_id)]['text'])\n",
    "        answers['answer_start'] = [passage_len + 4 + answers['answer_start'][0]] # insert four space when concatenate\n",
    "        \n",
    "    random_shuffles_answers[i] = answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_shuffles_answers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train)\n",
    "\n",
    "random_shuffles_df = train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = []\n",
    "\n",
    "for i, row in random_shuffles_df.iterrows():\n",
    "    if random_shuffles[i].index(gt_passage_table[i]) == 0:\n",
    "        context = origin_wiki[str(random_shuffles[i][0])]['text']\n",
    "        for j in range(1, len(random_shuffles[i])):\n",
    "            context += ' ' + wiki[str(random_shuffles[i][j])]['text']\n",
    "        contexts.append(context)\n",
    "    elif random_shuffles[i].index(gt_passage_table[i]) == 1:\n",
    "        context = wiki[str(random_shuffles[i][0])]['text']\n",
    "        context += ' ' + origin_wiki[str(random_shuffles[i][1])]['text']\n",
    "        for j in range(2, len(random_shuffles[i])):\n",
    "            context += ' ' + wiki[str(random_shuffles[i][j])]['text']\n",
    "        contexts.append(context)\n",
    "    elif random_shuffles[i].index(gt_passage_table[i]) == 2:\n",
    "        context = wiki[str(random_shuffles[i][0])]['text']\n",
    "        context += ' ' + wiki[str(random_shuffles[i][1])]['text']\n",
    "        context += ' ' + origin_wiki[str(random_shuffles[i][2])]['text']\n",
    "        for j in range(3, len(random_shuffles[i])):\n",
    "            context += ' ' + wiki[str(random_shuffles[i][j])]['text']\n",
    "        contexts.append(context)\n",
    "    elif random_shuffles[i].index(gt_passage_table[i]) == 3:\n",
    "        context = wiki[str(random_shuffles[i][0])]['text']\n",
    "        context += ' ' + wiki[str(random_shuffles[i][1])]['text']\n",
    "        context += ' ' + wiki[str(random_shuffles[i][2])]['text']\n",
    "        context += ' ' + origin_wiki[str(random_shuffles[i][3])]['text']\n",
    "        for j in range(4, len(random_shuffles[i])):\n",
    "            context += ' ' + wiki[str(random_shuffles[i][j])]['text']\n",
    "        contexts.append(context)\n",
    "    elif random_shuffles[i].index(gt_passage_table[i]) == 4:\n",
    "        context = wiki[str(random_shuffles[i][0])]['text']\n",
    "        context += ' ' + wiki[str(random_shuffles[i][1])]['text']\n",
    "        context += ' ' + wiki[str(random_shuffles[i][2])]['text']\n",
    "        context += ' ' + wiki[str(random_shuffles[i][3])]['text']\n",
    "        context += ' ' + origin_wiki[str(random_shuffles[i][4])]['text']\n",
    "        contexts.append(context)\n",
    "\n",
    "random_shuffles_df['context'] = contexts\n",
    "random_shuffles_df['answers'] = random_shuffles_answers.values()\n",
    "random_shuffles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in random_shuffles_df.iterrows():\n",
    "    if row['context'][row['answers']['answer_start'][0]] != row['answers']['text'][0][0]:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset.from_pandas(random_shuffles_df).save_to_disk('../passage_random_shuffle')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
