{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_from_disk\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/preprocess_wiki.json', 'r', encoding='utf-8') as f:\n",
    "    wiki = json.load(f)\n",
    "wiki['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [wiki[str(i)]['text'] for i in range(len(wiki))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_encoder = CrossEncoder('klue/bert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_encoder = SentenceTransformer('Huffon/sentence-klue-roberta-base')\n",
    "bi_encoder.max_seq_length = 384\n",
    "\n",
    "corpus_embeddings = bi_encoder.encode(corpus, convert_to_tensor=True, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "with open('/opt/ml/elastic_valid_500.bin','rb') as f:\n",
    "    elastic_valid = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, k):\n",
    "    bi_encoder_retrieval, cross_encoder_retrieval = [], []\n",
    "\n",
    "    ##### Sematic Search #####\n",
    "    # Encode the query using the bi-encoder and find potentially relevant passages\n",
    "    question_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
    "    question_embedding = question_embedding.cuda()\n",
    "    hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=100)\n",
    "    hits = hits[0]  # Get the hits for the first query\n",
    "\n",
    "    ##### Re-Ranking #####\n",
    "    # Now, score all retrieved passages with the cross_encoder\n",
    "    cross_inp = [[query, corpus[hit['corpus_id']]] for hit in hits]\n",
    "    cross_scores = cross_encoder.predict(cross_inp)\n",
    "\n",
    "    # Sort results by the cross-encoder scores\n",
    "    for idx in range(len(cross_scores)):\n",
    "        hits[idx]['cross-score'] = cross_scores[idx]\n",
    "\n",
    "    # Output of top-5 hits from bi-encoder\n",
    "    hits = sorted(hits, key=lambda x: x['score'], reverse=True)\n",
    "    for hit in hits[0:k]:\n",
    "        bi_encoder_retrieval.append(hit['corpus_id'])\n",
    "\n",
    "    hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
    "    for hit in hits[0:k]:\n",
    "        cross_encoder_retrieval.append(hit['corpus_id'])\n",
    "\n",
    "    return bi_encoder_retrieval, cross_encoder_retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = load_from_disk('../data/new_train_dataset')['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = search(td[0]['question'], 5)\n",
    "len(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_vd = load_from_disk('/opt/ml/data/train_dataset')['validation']\n",
    "len(origin_vd['document_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = origin_vd['question']\n",
    "question_embedding = bi_encoder.encode(question, convert_to_tensor=True)\n",
    "question_embedding = question_embedding.cuda()\n",
    "hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=500)\n",
    "\n",
    "st_v = {}\n",
    "for i, q in enumerate(question):\n",
    "    st_v[q] = hits[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sentence_transformer_valid.pickle', 'wb') as f:\n",
    "    pickle.dump(st_v, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = load_from_disk('../data/test_dataset')['validation']\n",
    "\n",
    "question = test_dataset['question']\n",
    "question_embedding = bi_encoder.encode(question, convert_to_tensor=True)\n",
    "question_embedding = question_embedding.cuda()\n",
    "hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=500)\n",
    "\n",
    "st_t = {}\n",
    "for i, q in enumerate(question):\n",
    "    st_t[q] = hits[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sentence_transformer_test.pickle', 'wb') as f:\n",
    "    pickle.dump(st_t, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(st_v[list(st_v.keys())[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(st_t[list(st_t.keys())[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = load_from_disk(\"/opt/ml/data/new_train_dataset/validation\")\n",
    "query = valid_dataset['question']\n",
    "context = valid_dataset['context']\n",
    "\n",
    "top_k_list = [20]\n",
    "\n",
    "for top_k in top_k_list:\n",
    "    es_acc = 0\n",
    "    bi_encoder_acc = 0\n",
    "    cross_encoder_acc = 0\n",
    "\n",
    "    for i in tqdm(range(len(query))):\n",
    "        q = query[i]\n",
    "        ground_truth = origin_vd[i]['document_id']\n",
    "\n",
    "        bi_encoder_top_k, cross_encoder_top_k = search(q, top_k)\n",
    "\n",
    "        es_top_k = []\n",
    "        for j in range(top_k):\n",
    "            es_top_k.append(elastic_valid[q][j])\n",
    "\n",
    "        if ground_truth in es_top_k:\n",
    "            es_acc += 1\n",
    "        if ground_truth in bi_encoder_top_k:\n",
    "            bi_encoder_acc += 1\n",
    "        if ground_truth in cross_encoder_top_k:\n",
    "            cross_encoder_acc += 1\n",
    "\n",
    "    print('score_top_k : ', top_k)\n",
    "    print('es ACC : ', es_acc / len(query))\n",
    "    print('bi-encoder ACC :',bi_encoder_acc / len(query))\n",
    "    print('cross-encoder ACC :',cross_encoder_acc / len(query))\n",
    "\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
